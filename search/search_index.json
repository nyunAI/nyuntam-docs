{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nyuntam","text":""},{"location":"#overview","title":"Overview","text":"<p> Nyuntam is NyunAI's advanced suite designed to optimize, adapt, and accelerate a wide array of deep learning models across various domains. The repository is structured into several submodules, each targeting specific tasks: </p> <ul> <li>NYUNTAM TEXT GENERATION: Focuses on compressing large language models for text generation tasks.</li> <li>NYUNTAM VISION : Tailored for compressing and optimizing vision models.</li> <li>NYUNTAM ADAPT: A robust module for fine-tuning and transfer learning across both vision and language models, leveraging state-of-the-art PEFT, full fine-tuning, and GPU parallelism.</li> </ul>"},{"location":"#setup-and-installation","title":"Setup and Installation","text":"<p> For hassle-free experimentation and quick results, Nyuntam provides a Command Line Interface tool:  nyunzero-cli. The documentation for using the CLI can be found  here. </p> <p> Nyuntam is a fully open-source project, and users are encouraged to explore the code and contribute to  nyuntam. </p>"},{"location":"#about-nyunai","title":"About NyunAI","text":"<p> NyunAI began its journey in 2020 with state-of-the-art research on model compression. Recognizing the challenges in  setting up pipelines for model compression and downstream task adaptation, NyunAI aimed to simplify the process  for researchers and developers, allowing them to efficiently build and deploy models while focusing entirely  on problem-solving. Since then, NyunAI has remained committed to developing efficient deep learning technology  and supporting software. </p>"},{"location":"#contact-nyunai","title":"Contact NyunAI","text":"<p> NyunAI's support team is always available to assist users with any questions or concerns. Users can reach out to  the team via email at support@nyunai.com. </p>"},{"location":"dataset/","title":"Dataset Importation","text":"<p>Zero provides comprehensive support for various custom dataset formats. Additionally, some Kompress Algorithms can execute without requiring any custom datasets.</p>"},{"location":"dataset/#dataset-preparation-guidelines","title":"Dataset Preparation Guidelines","text":""},{"location":"dataset/#image-classification","title":"Image Classification","text":"<p>For image classification tasks, Zero is compatible with the standard ImageNet dataset format. This format necessitates organizing images into folders representing respective categories, with validation splits already performed. The prescribed structure is as follows:</p> <pre><code>ImageNetDataset/\n| -- train\n|   | -- Class_A\n|   |   | -- image1.jpg\n|   |   | -- image2.jpg\n|   |   | -- ...\n|   | -- Class_B\n|   |   | -- image1.jpg\n|   |   | -- image2.jpg\n|   |   | -- ...\n|   | -- ...\n| -- val\n|   | -- Class_A\n|   |   | -- image1.jpg\n|   |   | -- image2.jpg\n|   |   | -- ...\n|   | -- Class_B\n|   |   | -- image1.jpg\n|   |   | -- image2.jpg\n|   |   | -- ...\n|   | -- ...\n</code></pre>"},{"location":"dataset/#object-detection","title":"Object Detection","text":"<p>Zero extends support to both COCO and VOC dataset formats for object detection tasks. The formats are detailed below:</p> <p>COCO Format</p> <pre><code>CocoDataset/\n| -- train2017\n|   | -- (Contains training images)\n| -- val2017\n|   | -- (Contains validation images)\n| -- annotations\n|   | -- instances_train2017.json  (Train Annotations)\n|   | -- instances_val2017.json    (Val Annotations)\n</code></pre> <p>VOC Format</p> <pre><code>VOCdevkit/\n| -- VOC2012\n|   | -- Annotations\n|   |   | -- (Contains annotation files)\n|   | -- JPEGImages\n|   |   | -- (Contains JPEG images)\n</code></pre>"},{"location":"dataset/#segmentation","title":"Segmentation","text":"<p>For image segmentation tasks, NYUN-KOMPRESS exclusively supports the VOC Format. The layout is structured as follows:</p> <pre><code>VOCdevkit/\n| -- VOC2012\n|   | -- Annotations\n|   |   | -- (Contains annotation files)\n|   | -- JPEGImages\n|   |   | -- (Contains JPEG images)\n|   | -- ImageSets\n|   |   | -- (Contains ImageSets files)\n|   | -- SegmentationClass\n|   |   | -- (Contains SegmentationClass files)\n|   | -- SegmentationObject\n|   |   | -- (Contains SegmentationObject files)\n</code></pre> <p>NYUN-Adapt requires the dataset to be in the following format : </p> <pre><code>custom_data/\n| -- dataset\n|   | -- images (images in jpg format)\n|   | -- labels (segmentation maps in png format)\n|   | -- splits \n|   |   | -- train.txt (names of the images for training (without the extension))\n|   |   | -- val.txt (names of the images for validation (without the extension))\n</code></pre>"},{"location":"dataset/#pose-detection","title":"Pose Detection","text":"<p>For pose detection tasks, Zero exclusively supports the COCO-Pose Format. The layout is structured as follows:</p> <pre><code>coco-pose/\n| -- images\n|   | -- train2017\n|   |   | -- Contains training images(JPEG format)\n|   | -- val2017\n|   |   | -- Contains testing images (JPEG images)\n| -- annotations\n|   |   | -- person_keypoints_train2017.json (Contains annotation file in coco-pose format)\n|   |   | -- person_keypoints_val2017.json (Contains annotation file in coco-pose format)\n</code></pre>"},{"location":"dataset/#llm-text-generation","title":"LLM Text Generation","text":"<p>Zero supports loading any text dataset compatible with the Hugging Face <code>datasets.load_dataset</code> for HF datasets or <code>datasets.load_from_disk</code> for custom datasets. It supports two main dataset formats:</p> <p>LLM - Single Column</p> <p>This format is suitable for use cases where the dataset is already formatted with a single text column. For example, \"wikitext\" dataset with \"text\" as the <code>TEXT_COLUMN</code>.</p> <p>LLM - Multi Columns</p> <p>This format is suitable for datasets with multiple columns. The multi-column dataset can also handle simple formatting of the dataset for instructional use cases. (See example usage below)</p> <p>Please note that there are no limitations regarding the loading of datasets with either single-column or multi-column structures. However, it is advisable to opt for loading multi-column datasets using option 2, especially when straightforward formatting is desired.</p> <p>Parameters for Dataset Loading:</p> Parameter Default Value Description DATASET_SUBNAME null Subname of the dataset if applicable. TEXT_COLUMN text Specifies the text columns to be used.If multiple columns are present, they should beseparated by commas. SPLIT train Specifies the split of the dataset to load,such as 'train', 'validation', or 'test'. FORMAT_STRING null If provided, this string isused to format the dataset.It allows for customizationof the dataset's text representation. <p>Dataset Formatting:</p> <p>The process responsible for formatting the dataset based on the provided parameters follows these steps:</p> <ol> <li>If no format string is provided, the <code>TEXT_COLUMN</code> and the dataset are used as is.</li> <li>If a format string is provided, it is applied to format the dataset. The format string should contain placeholders for the columns specified in <code>TEXT_COLUMN</code>.</li> <li>Once the dataset is formatted, a log message is generated to indicate that the format string was found and used. It also displays a sample of the formatted dataset.</li> <li>Finally, the dataset is mapped to replace the original \"text\" columns with the newly formatted \"text\" column (or) a new \"text\" column is created.</li> </ol> <p>Example usage for Multi-column dataset:</p> <p>Params for Alpaca Dataset (<code>yahma/alpaca-cleaned</code>)</p> <pre><code>DATASET_SUBNAME - null\nTEXT_COLUMN - input,output,instruction\nSPLIT - train\nFORMAT_STRING - Instruction:\\n{instruction}\\n\\nInput:\\n{input}\\n\\nOutput:\\n{output}\n</code></pre> <p>Alpaca dataset before Formatting</p> input instruction output How to bake a cake Step 1: Preheat the oven to... A delicious homemade cake... Introduction to Python Python is a high-level... Learn Python programming... History of the Roman Empire The Roman Empire was... Explore the rise and fall... ... ... ... <p>Alpaca dataset after formatting with the input params</p> input instruction output text How to bake a cake Step 1: Preheat the oven to... A delicious homemade cake... Instruction:\\nStep 1: Preheat...\\n\\nInput:\\nHow to bake a cake\\n\\nOutput:\\nA delicious homemade cake... Introduction to Python Python is a high-level... Learn Python programming... Instruction:\\nPython is a hi...\\n\\nInput:\\nIntroduction to Python\\n\\nOutput:\\nLearn Python programming... History of the Roman Empire The Roman Empire was... Explore the rise and fall... Instruction:\\nThe Roman Emp...\\n\\nInput:\\nHistory of the Roman Empire\\n\\nOutput:\\nExplore the rise and fall... ... ... ... ..."},{"location":"dataset/#text-classification","title":"Text Classification","text":"<p>Zero supports token classification and text clasification. Users can load any text dataset compatible with the Hugging Face <code>datasets.load_dataset</code> for HF datasets or <code>datasets.load_from_disk</code> for custom datasets. Make sure the uploaded dataset has the following format:</p> Parameter Data Type Default Value Description input_column str \"text\" The name of the input text column : \"tokens\" - for token classification (new, pos, chunk) and \"text\" - for text classification target_column str \"label\" the target column in the dataset : \"ner_tags\" : NER , \"pos_tags\": POS tagging, \"chunk_tags\": Chunking, \"label\" : text classification <p>Example dataset formats : </p> <p>TEXT CLASSIFICATION DATASET</p> text label I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also... 0 \"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken... 0 If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.One might... 0 This film was probably inspired by Godard's Masculin, f\u00e9minin and I urge you to see that film instead.The film has two strong elements and... 0 Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..\"Is that all there is?\"... 0 <p>TOKEN CLASSIFICATION DATASET</p> id tokens pos_tags 0 [\"EU\", \"rejects\", \"German\", \"call\", \"to\", ...] [22, 42, 16, 21, 35, 37, 16, 21, ...] 1 [\"Peter\", \"Blackburn\"] [22, 22] 2 [\"BRUSSELS\", \"1996-08-22\"] [22, 11] 3 [\"The\", \"European\", \"Commission\", \"said\", \"on\", ...] [12, 22, 22, 38, 15, 22, 28, 38, ...] 4 [\"Germany\", \"'s\", \"representative\", \"to\", ...] [22, 27, 21, 35, 12, 22, 22, 27, ...]"},{"location":"dataset/#summarization","title":"Summarization","text":"<p>Zero supports loading any text dataset compatible with the Hugging Face <code>datasets.load_dataset</code> for HF datasets or <code>datasets.load_from_disk</code> for custom datasets. Make sure the uploaded dataset has the following format:</p> document summary The full cost of damage in Newton Stewart, one of the areas worst affected, is still... Clean-up operations are continuing across the Scottish Borders and Dumfries and... A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST on Saturday... Two tourist buses have been destroyed by fire in a suspected arson attack in Belfas... Ferrari appeared in a position to challenge until the final laps, when the Mercedes... Lewis Hamilton stormed to pole position at the Bahrain Grand Prix ahead of Mercedes... John Edward Bates, formerly of Spalding, Lincolnshire, but now living in London,... A former Lincolnshire Police officer carried out a series of sex attacks on... Patients and staff were evacuated from Cerahpasa hospital on Wednesday after a man... An armed man who locked himself into a room at a psychiatric hospital in Istanbul has... <p>The dataset params for summarization tasks in Adapt are as follows : </p> Key Value Description DATASET_SUBNAME null Subname of the dataset if applicable. input_column 'document' Name of the input column containing the text corpus target_column 'summary' Name of the target column containing the summarized text"},{"location":"dataset/#question-answering","title":"Question Answering","text":"<p>The dataset for question answering must follow the general Adapt dataset. Adapt currently support extractive question-answering and hence requires :</p> <ul> <li>CONTEXT          - Text column that contains the context </li> <li>QUESTION         - Contains the Question </li> <li>ANSWER         - A column containing dictionary entries with the answer and index of context from which the answer starts.          example :          <code>{ \"text\": [ \"...answer text...\" ], \"answer_start\": [ 276 ] }</code></li> </ul> <p>The column names can be different from this, but they should be mentioned in </p> <ul> <li>input_column : 'context'</li> <li>input_question_column : 'question'</li> <li>target_column : 'answer'</li> </ul> <p>Dataset arguments in question answering: </p> Key Value Description DATASET_SUBNAME null Subname of the dataset if applicable. input_column 'context' Name of the input column input_question_column 'question' Name of the input question column target_column 'answer' Name of the target column squad_v2_format False #True Whether the data follows SQUAD-V2 Format (True/False) <p>Example of a default dataset for Question Answering :</p> context question answers Beyonc\u00e9 Giselle Knowles-Carter ... When did Beyonce start becoming... { \"text\": [ \"in the late 1990s\" ],...} Beyonc\u00e9 Giselle Knowles-Carter What areas did Beyonce compete in... { \"text\": [ \"singing and dancing\" ],... } Beyonc\u00e9 Giselle Knowles-Carter When did Beyonce leave Destiny's Chil... { \"text\": [ \"2003\" ], \"answer_start\": [ 526 ...] } Beyonc\u00e9 Giselle Knowles-Carter In what city and state did Beyonce... { \"text\": [ \"Houston, Texas\" ],... }"},{"location":"dataset/#translation","title":"Translation","text":"<p>Zero supports loading any text dataset compatible with the Hugging Face <code>datasets.load_dataset</code> for HF datasets or <code>datasets.load_from_disk</code> for custom datasets. Make sure the uploaded dataset has the following format:</p> id translation 0 { \"ca\": \"Source: Project GutenbergTranslation: Josep Carner\", \"de\": \"Source: Project Gutenberg\" } 1 { \"ca\": \"Les Aventures De Tom Sawyer\", \"de\": \"Die Abenteuer Tom Sawyers\" } 2 { \"ca\": \"Mark Twain\", \"de\": \"Mark Twain\" } 3 { \"ca\": \"PREFACI.\", \"de\": \"Vorwort des Autors.\" } 4 { \"ca\": \"La major part de les aventures consignades en aquest llibre succe\u00efren de bo de bo; una o dues s\u00f3n experiments de la meva collita; la resta pertanye...\", \"de\": \"\" } <p>Dataset Arguments in Translation:</p> Key Value Description source_lang '' The key of source language as per the given dataset target_lang '' The key of target language as per the given dataset DATASET_SUBNAME null Subset for the dataset (for multilingual datasets, the subname generally represents the pair of language used for translation)"},{"location":"dataset/#importing-your-dataset","title":"Importing Your Dataset","text":"<p>There are two different ways to import your dataset into Zero:</p>"},{"location":"dataset/#custom-data","title":"Custom Data","text":"<p>Users who are using a custom dataset (stored locally) to finetune a model can use the <code>CUSTOM_DATASET_PATH</code> argument in the yaml to do so. </p>"},{"location":"dataset/#pre-existing-dataset","title":"Pre-existing dataset","text":"<p>Users who are using existing datasets from huggingface can use the <code>DATASET</code> argument in the yaml. </p> <p>For more examples please refer to Examples </p> <p>Note: For LLM tasks, the data folder must be loadable by <code>datasets.load_from_disk</code> and should return a <code>datasets.DatasetDict</code> object and not a <code>datasets.Dataset</code> object.</p>"},{"location":"nyunzero_cli/","title":"Nyun CLI","text":"<p>Nyun CLI is a command-line interface tool that provides a convenient way to initialize and manage your Nyun workspace, as well as run various scripts and algorithms supported by Nyun.</p>"},{"location":"nyunzero_cli/#installation","title":"Installation","text":"<p>To install Nyun CLI, you need to have Python 3.6 or later installed on your system. You can then install the CLI tool using pip:</p> <pre><code>pip install nyun-cli\n</code></pre>"},{"location":"nyunzero_cli/#usage","title":"Usage","text":"<p>After installation, you can use the <code>nyun</code> command to access the available commands. Run <code>nyun --help</code> to see the list of available commands and their descriptions.</p>"},{"location":"nyunzero_cli/#initializing-the-workspace","title":"Initializing the Workspace","text":"<p>Before you can run any scripts or algorithms, you need to initialize your Nyun workspace. You can do this using the <code>init</code> command:</p> <pre><code>nyun init [WORKSPACE_PATH] [CUSTOM_DATA_PATH] [OPTIONS]\n</code></pre> <ul> <li><code>WORKSPACE_PATH</code>: The path to the workspace directory. If not provided, the current working directory will be used.</li> <li><code>CUSTOM_DATA_PATH</code>: The path to the custom data directory. If not provided, a default directory will be created within the workspace.</li> <li><code>OPTIONS</code>:</li> <li><code>--overwrite</code>, <code>-o</code>: Overwrite the existing workspace spec if it already exists.</li> <li><code>--extensions</code>, <code>-e</code>: Specify the extensions to install. Defaults to installing all available extensions. Available extensions are:<ul> <li><code>kompress-vision</code>: For vision-related tasks (e.g., object detection, image classification), using Nyun Kompress.</li> <li><code>kompress-text-generation</code>: For text generation tasks using Nyun Kompress.</li> <li><code>adapt</code>: For the Adapt framework, which supports various tasks like detection, segmentation, and text generation.</li> <li><code>all</code>: Install all available extensions.</li> <li><code>none</code>: Don't install any extension.</li> </ul> </li> </ul> <p>Example:</p> <pre><code># mkdir ~/my-workspace\nnyun init ~/my-workspace ~/my-data --extensions kompress-vision\n</code></pre> <p>This command initializes a new workspace at <code>~/my-workspace</code> and sets the custom data directory to <code>~/my-data</code>, installing the Nyun Kompress Vision extension.</p>"},{"location":"nyunzero_cli/#running-scripts","title":"Running Scripts","text":"<p>Once your workspace is initialized, you can run scripts using the <code>run</code> command in the workspace directory. The command syntax is as follows:</p> <pre><code>nyun run [SCRIPT_PATH]\n</code></pre> <ul> <li><code>SCRIPT_PATH</code>: The path(s) to the YAML or JSON script file you want to run.</li> </ul> <p>Example:</p> <pre><code>nyun run ~/my-script.yaml\n</code></pre> <p>This command runs the script located at <code>~/my-script.yaml</code> within your initialized workspace.</p> <p>To run chained scripts, you can provide multiple script paths in the order of execution:</p> <pre><code>nyun run ~/my-script1.yaml ~/my-script2.yaml\n</code></pre>"},{"location":"nyunzero_cli/#checking-version","title":"Checking Version","text":"<p>To check the version of the Nyun CLI you have installed, use the <code>version</code> command:</p> <pre><code>nyun version\n</code></pre>"},{"location":"support/","title":"Model Compression &amp; Adaption Support Grid","text":"<p>Below are tables summarizing the support for various compression and adaption techniques across different models. Apart from these, other similar models may be supported but have not been tested. </p>"},{"location":"support/#vision-compression","title":"Vision Compression","text":"<p>Note that in the Table below, CPU and GPU indicate the target device of deployment. PTQ indicates Post Training Quantization and QAT indicates Quantization Aware Training.</p> Model CPU PTQ - Torch CPU PTQ - OpenVino CPU PTQ - ONNX GPU PTQ - TensorRT CPU QAT - Torch Knowledge Distillation Structured Pruning CPU QAT - OpenVino Resnet (timm) \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Convnextv2 (huggingface) - \u2713 \u2713 \u2713 - \u2713 \u2713 \u2713 Mobilenetv3 (timm) \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 DeiT (huggingface) \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 VanillaNet (timm) - - \u2713 \u2713 - \u2713 \u2713 \u2713 Swin (huggingface) \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 YoloX (mmyolo/mmdet) - \u2713 - \u2713 \u2713 \u2713 \u2713 - RTMDet (mmyolo/mmdet) - \u2713 - \u2713 \u2713 \u2713 - - Yolov8 (mmyolo) - - - \u2713 - \u2713 - -"},{"location":"support/#llm-compression","title":"LLM Compression","text":"Model AWQ LMQuant (QoQ) AQLM TensorRT Exllama MLC-LLM FLAP LLaMA \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 LlaMA-2 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Llama-3 \u2713 \u2713 \u2713 - \u2713 - \u2713 Vicuna \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Mistral \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Mixtral \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemma \u2713 - \u2713 \u2713 - - -"},{"location":"support/#adapt","title":"Adapt","text":""},{"location":"support/#llm-tasks","title":"LLM Tasks","text":"<ul> <li> Text Generation </li> <li> Summarization </li> <li> Question Answering </li> <li> Text Classification </li> <li>Translation </li> </ul> <p>All of the major huggingface models are supported for these tasks.</p>"},{"location":"support/#image-classification","title":"Image Classification","text":"<p>All major image models on huggingface and timm are supported in Adapt.</p>"},{"location":"support/#object-detection","title":"Object Detection","text":"LoRA SSF DoRA Full Fine Tuning YoloX \u2713 \u2713 \u2713 \u2713 RTMDet \u2713 \u2713 \u2713 \u2713"},{"location":"support/#instance-segmentation","title":"Instance Segmentation","text":"LoRA SSF DoRA Full Fine Tuning SegNeXT \u2713 \u2713 \u2713 \u2713"},{"location":"support/#pose-detection","title":"Pose Detection","text":"LoRA SSF DoRA Full Fine Tuning RTMO \u2713 \u2713 \u2713 \u2713 <p>Note that quantization support (QLoRA/QSSF) for adaptation of vision models is currently not supported.</p>"},{"location":"examples/","title":"Fast, Small and Efficient AI","text":"<p>At NyunAI, we are committed to developing fast, small, and efficient AI models that can be deployed in datacenters, inference-servers &amp; edge devices with limited resources. Our examples demonstrate various compression and adaption techniques for Deep Learning models, focusing on reducing model size and improving inference speed while maintaining accuracy.</p> <p>This directory contains a collection of examples demonstrating multiple techniques applied on various Computer Vision, NLP, and GenAI models.</p>"},{"location":"examples/#text-generation","title":"Text Generation","text":""},{"location":"examples/#1-maximising-math-performance-for-extreme-compressions-2-bit-llama3-8b-w2a16","title":"1. Maximising math performance for extreme compressions: 2-bit Llama3-8b (w2a16)","text":"<p>This guide provides a detailed walkthrough on maximizing the performance of a highly compressed Llama3-8b model using 2-bit weights and 16-bit activations. We will apply the Additive Quantization for Large Models (AQLM) technique to compress and optimize the Llama3-8b model, drastically reducing its memory footprint while maintaining performance.</p>"},{"location":"examples/#2-efficient-4-bit-quantization-w4a16-of-llama31-8b-for-optimized-text-generation","title":"2. Efficient 4-bit Quantization (w4a16) of Llama3.1-8b for Optimized Text Generation","text":"<p>This guide provides a walkthrough of applying AWQ (Activation-aware Weight Quantization) to compress and accelerate the Llama3.1-8b model using 4-bit weights and 16-bit activations. AWQ allows for significant reduction in model size and computational requirements without sacrificing performance, making it an excellent choice for deploying large language models in resource-constrained environments.</p>"},{"location":"examples/#3-llama31-70b-05x-the-cost-size","title":"3. Llama3.1 70B: 0.5x the cost &amp; size","text":"<p>This guide provides a walkthrough of applying FLAP (Fluctuation-based Adaptive Structured Pruning) to compress and accelerate the Llama3.1-70b model. FLAP allows for significant reduction in model size and computational requirements without sacrificing performance. Unlike traditional pruning techniques, FLAP requires no retraining and adapts the pruning ratio across different modules and layers, offering an efficient and effective approach for deploying large language models in resource-constrained environments.</p>"},{"location":"examples/#4-achieving-up-to-25x-tensorrtllm-speedups-efficient-4-8-4-quantization-w4a8kv4-of-llama31-8b","title":"4. Achieving Up to 2.5x TensorRTLLM Speedups: Efficient 4-8-4 Quantization (w4a8kv4) of Llama3.1-8b","text":"<p>This guide provides a detailed walkthrough of applying LMQuant using the QoQ algorithm (quattuor-octo-quattuor) to quantize the Llama3.1-8b model. By using 4-bit weights, 8-bit activations, and 4-bit key-value cache (W4A8KV4), LMQuant aims to significantly reduce model size while maintaining high performance and efficient inference speed. This process is particularly beneficial for deploying large language models in environments with limited resources.</p>"},{"location":"examples/#5-accelerating-a-4-bit-quantised-llama-model","title":"5. Accelerating a 4-bit Quantised Llama Model","text":"<p>This guide demonstrates how to accelerate a 4-bit quantized (awq) Llama model with the TensorRTLLM engine. TensorRTLLM is a high-performance inference engine that leverages NVIDIA's TensorRT library to optimize and accelerate models for deployment on NVIDIA GPUs.</p>"},{"location":"examples/#computer-vision","title":"Computer Vision","text":""},{"location":"examples/#1-pruning-yolox-with-mmrazor","title":"1. Pruning YOLOX with MMRazor","text":"<p>Pruning Object Detection is supported via MMRazor Pruning. Currently activation based pruning and flop based pruning are the available option. This example utilizes NyunCLI a quick and seamless solution to run the pruning job.</p>"},{"location":"examples/#2-8-bit-cpu-quantization-of-resnet50-using-nncf-on-cifar-10-dataset","title":"2. 8-bit CPU Quantization of ResNet50 using NNCF on CIFAR-10 Dataset","text":"<p>8-bit quantization of Image Classification models are done via NNCF, ONNX Quantizers for CPU and TensorRT Quantizer for GPU deployment. This example shows how to Quantize a ResNet50 model with NNCF for CPU Quantization.</p>"},{"location":"examples/#adaption","title":"Adaption","text":""},{"location":"examples/#1-adapting-segnext-to-cityscapes-dataset-using-ssf","title":"1. Adapting SegNeXt to cityscapes dataset using SSF","text":"<p>This guide provides a walkthrough of applying SegNeXt for instance segmentation on the cityscapes dataset using SSF (Scaling and Shifting the deep Features). SSF enables parameter efficient fine-tuning by proposing that performace simillar to full fine-tuning can be achieved by only scaling and shifting the features of a deep neural network.</p>"},{"location":"examples/#2-finetuning-rtmdet-on-face-det-dataset-using-lora-and-ddp","title":"2. Finetuning RTMDet on face-det dataset using LoRA and DDP","text":"<p>This guide provides a walkthrough of applying RTMDet for face detection on the face-det dataset using LoRA (Low-Rank Adaptation) with Distributed Data Parallel (DDP) across 2 GPUs. LoRA enables efficient fine-tuning by reducing the memory footprint, making it a powerful approach for high-performance face detection while maintaining scalability and resource efficiency.</p>"},{"location":"examples/#3-finetuning-t5-large-with-qlora-on-xsum-dataset","title":"3. Finetuning T5 large with QLoRA on XSUM dataset","text":"<p>This guide provides a detailed walkthrough for finetuning T5 Large model on the xsum Dataset with QLoRA using nyuntam-adapt. QLoRA is a PEFT technique where the original weights are frozen to reduce the trainable parameters and and qre quantized to reduce the memory usage.</p>"},{"location":"examples/#4-finetuning-llama3-8b-with-qdora-and-fsdp","title":"4. Finetuning Llama3-8b with QDoRA and FSDP","text":"<p>In this example we will be finetuning Llama3-8b with QDoRA and FSDP. We will be using the the Llama-1k dataset for this example but any dataset can be used for this purpose. DoRA is a PEFT method which, simillar to LoRA, trains adapters by freezing and quantizing the original model weights.</p>"},{"location":"examples/adapt/image_segmentation/","title":"SegNeXt on cityscapes dataset using SSF","text":""},{"location":"examples/adapt/image_segmentation/#overview","title":"Overview","text":"<p>This guide provides a walkthrough of applying SegNeXt for instance segmentation on the cityscapes dataset using SSF (Scaling and Shifting the deep Features). SSF enables parameter efficient fine-tuning by proposing that performace simillar to full fine-tuning can be achieved by only scaling and shifting the features of a deep neural network. </p>"},{"location":"examples/adapt/image_segmentation/#table-on-contents","title":"Table on Contents","text":"<ul> <li>Introduction</li> <li>Requirements</li> <li>Installation</li> <li>Dataset</li> <li>Configuration</li> <li>Adapting the model</li> <li>Conclusion</li> </ul>"},{"location":"examples/adapt/image_segmentation/#introduction","title":"Introduction","text":"<p>In this example we will be finetuning SegNeXt large model on a city scapes dataset uisng a PEFT technique called SSF (Scale and Shift deep Features). SSF uses two linear layers to learn the scale and shift factors for deep features and hence uses very few trainable parameters. </p>"},{"location":"examples/adapt/image_segmentation/#requirements","title":"Requirements","text":"<p>Before you begin, ensure that you have the following: - A GPU-enabled environment with CUDA support. - The Nyuntam repository cloned and set up as per the Installation Guide. - Docker</p>"},{"location":"examples/adapt/image_segmentation/#installation","title":"Installation","text":""},{"location":"examples/adapt/image_segmentation/#step-1-clone-the-nyuntam-repository","title":"Step 1: Clone the Nyuntam Repository","text":"<p>Clone the repository and navigate to the <code>nyuntam</code> directory:</p> <pre><code>$ git clone https://github.com/nyunAI/nyuntam.git\n$ cd nyuntam\n</code></pre>"},{"location":"examples/adapt/image_segmentation/#step-2-set-up-the-workspace","title":"Step 2: Set Up the workspace","text":"<p>To setup the environment use the following command(s),</p> <pre><code>pip install git+https://github.com/nyunAI/nyunzero-cli.git\nnyun init {WORKSPACE_PATH} -e adapt\n</code></pre>"},{"location":"examples/adapt/image_segmentation/#dataset","title":"Dataset","text":"<p>For this experiment we will be using the CityScapes dataset. The original folder structure of the dataset loks like :  </p> <pre><code>iccv09Data\n\u2514\u2500\u2500 images\n|   \u2514\u2500\u2500 image1.jpg\n|   \u2514\u2500\u2500 image2.jpg\n|   \u2514\u2500\u2500 image3.jpg\n    ...\n\u2514\u2500\u2500 seg_maps\n|   \u2514\u2500\u2500 image1.layers.txt\n|   \u2514\u2500\u2500 image1.regions.txt\n|   \u2514\u2500\u2500 image1.surfaces.txt\n\u2514\u2500\u2500 ann_file.txt\n\u2514\u2500\u2500 horizons.txt\n</code></pre> <p>The text files contain the segmentation maps in digits. In this dataset we will only be using the \"regions\" as the segmentation maps. These are the text files which are named as \"image1.regions.txt\". These text files are converted to \".png\" images. The new data folder structure is formatted in the folowing way: </p> <pre><code>iccv09Data\n\u2514\u2500\u2500 images\n|   \u2514\u2500\u2500 image1.jpg\n|   \u2514\u2500\u2500 image2.jpg\n|   \u2514\u2500\u2500 image3.jpg\n    ...\n\u2514\u2500\u2500 seg_maps\n    \u2514\u2500\u2500 image1.png\n    \u2514\u2500\u2500 image2.png\n    \u2514\u2500\u2500 image3.png\n    ...\n\u2514\u2500\u2500train_ann.txt (text file containing the names of training images without the extension)\n\u2514\u2500\u2500val_ann.txt (text file containing the names of validation images without the extension)\n</code></pre>"},{"location":"examples/adapt/image_segmentation/#configuration","title":"Configuration","text":"<p>The following YAML file is used for setting up the experiment : </p> <pre><code>\n#DATASET\nJOB_SERVICE: Adapt\nCUSTOM_DATASET_PATH : \"/custom_data/iccv09Data\" \n\n\nMODEL : 'segnext' \nMODEL_PATH :  'segnext_mscan-t_1xb16-adamw-160k_ade20k-512x512'  # config of model in case of mmseg\nCACHE_BOOL : False\nLOCAL_MODEL_PATH: #This is empty becasue we are using the pretrained model from the internet. \n\n\nSEED : 56\nDO_TRAIN : True\nDO_EVAL : False\nNUM_WORKERS : 4\nBATCH_SIZE : 1\nEPOCHS : 1\nSTEPS : \nOPTIMIZER : 'AdamW' \nLR : 5e-4\nSCHEDULER_TYPE : 'MultiStepLR'\nWEIGHT_DECAY : 0.0\nBETA1 : 0.9\nBETA2 : 0.999\nADAM_EPS : 1e-8 \nINTERVAL : 'steps'\nINTERVAL_STEPS : 4      #For interval based training loops in mmlabs \nNO_OF_CHECKPOINTS : 5\nFP16 : False\nRESUME_FROM_CHECKPOINT : False \nGRADIENT_ACCUMULATION_STEPS : 3\nGRADIENT_CHECKPOINTING : False\nBEGIN: 0\nEND: 50\n\n#MMSEG SPECIFIC ARGUMENTS\namp :  False\nresume  : #False # ['auto']\nauto_scale_lr :  False\ncfg_options  : None\nlauncher :  none\ndest_root : './.mmseg_cache'   #This saves the checkpoints and .py files for the model configs and mmseg logs   \ntrain_ann_file : \"train_ann_file.txt\" \nval_ann_file : \"val_ann_file.txt\" \nwork_dir : ./results/mmseg  # same as output dir\ncheckpoint_interval : 45\ntrain_img_file: images  #For MMSEG - Folder containing training images\ntrain_seg_file: seg_maps  #For MMSEG - Folder containing training segmentation maps\nval_img_file: images   #For MMSEG - Folder containing validation images\nval_seg_file: seg_maps     #For MMSEG - Folder containing validation segmentation maps\nnum_classes: 8\nclass_list: ['sky', 'tree', 'road', 'grass', 'water', 'bldg', 'mntn', 'fg obj'] #List containing all class names\npalette: [[128, 128, 128], [129, 127, 38], [120, 69, 125], [53, 125, 34], \n           [0, 11, 123], [118, 20, 12], [122, 81, 25], [241, 134, 51]]   #List of lists contaning class colors - [[r,g,b],[r,g,b],[r,g,b]]\n\n\nLAST_LAYER_TUNING : True\nFULL_FINE_TUNING : False\n\nPEFT_METHOD : 'SSF'\npeft_type : 'SSF'\n\n\nLibrary :  'MMSEG'\ncuda_id : '0'\nOUTPUT_DIR : \"/user_data/jobs/Adapt/IMGSEG\"\nOVERWRITE_OUTPUT_DIR : False\nLOGGING_PATH : \"/user_data/logs/Adapt/IMGSEG\" \nMERGE_ADAPTER: False\nTASK : 'image_segmentation'\nauto_select_modules: True\nSAVE_METHOD : 'state_dict'\n\n</code></pre>"},{"location":"examples/adapt/image_segmentation/#adapting-the-model","title":"Adapting the model","text":"<p>With the yaml file configured, the adaptation process is initiated with the following command : </p> <pre><code>nyun run examples/adapt/image_segmentation/config.yaml\n</code></pre> <p>Once the job starts, you will find the following directory structure in the <code>user_data</code> folder:</p> <pre><code>user_data/\n\u251c\u2500\u2500 jobs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Adapt\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 IMGSEG\n\u251c\u2500\u2500 logs\n \u00a0\u00a0 \u2514\u2500\u2500 Adapt\n  \u00a0     \u2514\u2500\u2500 IMGSEG\n  \u00a0         \u2514\u2500\u2500 log.log\n\n</code></pre> <p>The output model will be stored in <code>user_data/jobs/Adapt/IMGSEG/</code> directory and the final directory structure will be:</p> <pre><code>user_data/\n\u251c\u2500\u2500 jobs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Adapt\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 IMGSEG\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 merged_model_state_dict.pth\n\u251c\u2500\u2500 logs\n \u00a0\u00a0 \u2514\u2500\u2500 Adapt\n  \u00a0     \u2514\u2500\u2500 IMGSEG\n  \u00a0         \u2514\u2500\u2500 log.log\n\n</code></pre>"},{"location":"examples/adapt/image_segmentation/#conclusion","title":"Conclusion","text":"<p>This guide has walked you through the process of adapting the SegNext model using SSF for instance segmentation of image regions on the cityscapes dataset. By employing SSF, we efficiently fine-tuned the model with a reduced memory footprint. The configuration and setup steps were outlined, ensuring that even complex tasks like distributed training and low-rank adaptation are manageable. The final trained model and logs are organized in a clear directory structure, making it easy to retrieve and analyze results.</p> <p>Author: Panigrahi, Abhranta</p>"},{"location":"examples/adapt/object_detection/","title":"Finetuning RTMDet on face-det dataset using LoRA and DDP","text":""},{"location":"examples/adapt/object_detection/#overview","title":"Overview","text":"<p>This guide provides a walkthrough of applying RTMDet for face detection on the face-det dataset using LoRA (Low-Rank Adaptation) with Distributed Data Parallel (DDP) across 2 GPUs. LoRA enables efficient fine-tuning by reducing the memory footprint, making it a powerful approach for high-performance face detection while maintaining scalability and resource efficiency.</p>"},{"location":"examples/adapt/object_detection/#table-on-contents","title":"Table on Contents","text":"<ul> <li>Introduction</li> <li>Requirements</li> <li>Installation</li> <li>Dataset</li> <li>Configuration</li> <li>Adapting the model</li> <li>Conclusion</li> </ul>"},{"location":"examples/adapt/object_detection/#introduction","title":"Introduction","text":"<p>In this example, we'll be applying the LoRA technique to fine-tune the RTMDet model on the Face-Det dataset. The training process will be distributed across 2 GPUs using Distributed Data Parallel (DDP) to maximize efficiency. LoRA's ability to introduce low-rank adaptations allows for targeted model updates, significantly reducing memory and computational requirements while increasing (Out Of Distribution) OOD generalization performance. </p>"},{"location":"examples/adapt/object_detection/#requirements","title":"Requirements","text":"<p>Before you begin, ensure that you have the following: - A GPU-enabled environment with CUDA support. - The Nyuntam repository cloned and set up as per the Installation Guide. - Docker</p>"},{"location":"examples/adapt/object_detection/#installation","title":"Installation","text":""},{"location":"examples/adapt/object_detection/#step-1-clone-the-nyuntam-repository","title":"Step 1: Clone the Nyuntam Repository","text":"<p>Clone the repository and navigate to the <code>nyuntam</code> directory:</p> <pre><code>$ git clone https://github.com/nyunAI/nyuntam.git\n$ cd nyuntam\n</code></pre>"},{"location":"examples/adapt/object_detection/#step-2-set-up-the-workspace","title":"Step 2: Set Up the workspace","text":"<p>To setup the environment use the following command(s),</p> <pre><code>pip install git+https://github.com/nyunAI/nyunzero-cli.git\nnyun init {WORKSPACE_PATH} -e adapt\n</code></pre>"},{"location":"examples/adapt/object_detection/#dataset","title":"Dataset","text":"<p>For this experiment we will be using the face-det dataset from roboflow. The dataset is downloaded in coco format. The data folder structure is formatted in the folowing way: </p> <pre><code>face-det\n\u2514\u2500\u2500 train\n|   \u2514\u2500\u2500 annotations_coco.json\n|   \u2514\u2500\u2500 image1.jpg\n|   \u2514\u2500\u2500 image2.jpg\n    ...\n\u2514\u2500\u2500 validation\n    \u2514\u2500\u2500 annotations_coco.json\n    \u2514\u2500\u2500 image1.jpg\n    \u2514\u2500\u2500 image2.jpg\n\n</code></pre>"},{"location":"examples/adapt/object_detection/#configuration","title":"Configuration","text":"<p>The following YAML file is used for setting up the experiment : </p> <pre><code>#DATASET_ARGS :\nJOB_SERVICE: Adapt\nTRAIN_DIR : 'train/'\nVAL_DIR : 'validation/'\nTEST_DIR : \nCUSTOM_DATASET_PATH : \"/custom_data/face_det\"\n\n#MODEL_ARGS :\nMODEL : 'rtmdet' \nMODEL_PATH :  'rtmdet_tiny_8xb32-300e_coco'  # config of model in case of mmdet\nCACHE_BOOL : False\nLOCAL_MODEL_PATH:  #This is empty becasue we are using the pretrained model from the internet. \n\n#TRAINING_ARGS :\nSEED : 56\nDO_TRAIN : True\nDO_EVAL : True\nNUM_WORKERS : 4\nBATCH_SIZE : 1\nEPOCHS : 1\nOPTIMIZER : 'SGD' \nLR : 5e-3 \nSCHEDULER_TYPE : 'CosineAnnealingLR'\nWEIGHT_DECAY : 0.0\nINTERVAL : 'steps'\nINTERVAL_STEPS : 4\nNO_OF_CHECKPOINTS : 5\nFP16 : False\nGRADIENT_ACCUMULATION_STEPS : 4\nGRADIENT_CHECKPOINTING : False\nBEGIN: 0\nEND: 50\n\n#MMDET SPECIFIC ARGUMENTS :\namp :  False\nresume  : False # ['auto']\nauto_scale_lr :  False\ncfg_options  : None\nlauncher :  pytorch\ndest_root : './.mmdet_cache'       \ntrain_ann_file : 'annotations.coco.json'     \nval_ann_file : 'annotations.coco.json'        \nwork_dir : ./results/mmdet  # same as output dir\ncheckpoint_interval : 5\n\n#FINE_TUNING_ARGS :\nLAST_LAYER_TUNING : True\nFULL_FINE_TUNING :  False\n\nPEFT_METHOD : \"LoRA\"\n\n#LoRA_CONFIG :\nr : 32\nalpha : 16\ndropout : 0.1\npeft_type : 'LoRA'\nfan_in_fan_out : False\ninit_lora_weights : True  \n\nTASK : 'object_detection'\nLibrary :  'MMDET' \ncuda_id : '0,1'\nOUTPUT_DIR : \"/user_data/jobs/Aadpt/OBJDET\"\nLOGGING_PATH : \"/user_data/logs/Adapt/OBJDET\"\nMERGE_ADAPTER: True\nauto_select_modules: True\nSAVE_METHOD : 'state_dict'\n\n#DDP ARGS\nDDP: True\nnum_nodes: 1\n</code></pre>"},{"location":"examples/adapt/object_detection/#adapting-the-model","title":"Adapting the model","text":"<p>With the yaml file configured, the adaptation process is initiated with the following command : </p> <pre><code>nyun run examples/adapt/object_detection/config.yaml\n</code></pre> <p>Once the job starts, you will find the following directory structure in the <code>/user_data</code> folder:</p> <pre><code>/user_data/\n\u251c\u2500\u2500 jobs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Adapt\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 OBJDET\n\u251c\u2500\u2500 logs\n \u00a0\u00a0 \u2514\u2500\u2500 Adapt\n  \u00a0     \u2514\u2500\u2500 OBJDET\n  \u00a0         \u2514\u2500\u2500 log.log\n\n</code></pre> <p>LoRA decreases the total trainable parameters of the model by freezing the original model weights and updating LoRA adapters only. </p> <pre><code>08/05/2024 23-04-20 - INFO - nyuntam_adapt.core.base_algorithm - trainable params: 1491594 || all params: 6009438 || trainable%: 24.82\n</code></pre> <p>This is a sample of the experiment logs  : </p> <pre><code>08/05/2024 23-04-34 - INFO - stdout - 08/05 23:04:34 - mmengine - INFO - Epoch(train) [1][ 150/2988]  base_lr: 5.0000e-03 lr: 5.0000e-03  eta: 0:03:39  time: 0.0673  data_time: 0.0008  memory: 220  loss: 2.0014  loss_cls: 0.9983  loss_bbox: 1.0031\n\n08/05/2024 23-04-37 - INFO - stdout - 08/05 23:04:37 - mmengine - INFO - Epoch(train) [1][ 200/2988]  base_lr: 5.0000e-03 lr: 5.0000e-03  eta: 0:03:29  time: 0.0683  data_time: 0.0008  memory: 220  loss: 1.8670  loss_cls: 0.8558  loss_bbox: 1.0112\n\n08/05/2024 23-04-40 - INFO - stdout - 08/05 23:04:40 - mmengine - INFO - Epoch(train) [1][ 250/2988]  base_lr: 5.0000e-03 lr: 5.0000e-03  eta: 0:03:22  time: 0.0695  data_time: 0.0008  memory: 220  loss: 1.7820  loss_cls: 0.8524  loss_bbox: 0.9297\n\n</code></pre> <p>The output model will be stored in <code>/user_data/jobs/Adapt/100/</code> directory and the final directory structure will be:</p> <pre><code>/user_data/\n\u251c\u2500\u2500 jobs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Adapt\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 OBJDET\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 merged_model_state_dict.pth\n\u251c\u2500\u2500 logs\n \u00a0\u00a0 \u2514\u2500\u2500 Adapt\n  \u00a0     \u2514\u2500\u2500 OBJDET\n  \u00a0         \u2514\u2500\u2500 log.log\n\n</code></pre>"},{"location":"examples/adapt/object_detection/#conclusion","title":"Conclusion","text":"<p>This guide has walked you through the process of adapting the RTMDet model using LoRA for face detection on the Face-Det dataset, leveraging Distributed Data Parallel (DDP) across two GPUs. By employing LoRA, we efficiently fine-tuned the model with a reduced memory footprint, allowing for scalable and high-performance object detection. The configuration and setup steps were outlined, ensuring that even complex tasks like distributed training and low-rank adaptation are manageable. The final trained model and logs are organized in a clear directory structure, making it easy to retrieve and analyze results.</p> <p>Author: Panigrahi, Abhranta</p>"},{"location":"examples/adapt/summarization/","title":"Finetuning T5 large with QLoRA on XSUM dataset","text":""},{"location":"examples/adapt/summarization/#overview","title":"Overview","text":"<p>This guide provides a detailed walkthrough for finetuning T5 Large model on the xsum Dataset with QLoRA using nyuntam-adapt. QLoRA is a PEFT technique where the original weights are frozen to reduce the trainable parameters and and qre quantized to reduce the memory usage. </p>"},{"location":"examples/adapt/summarization/#table-on-contents","title":"Table on Contents","text":"<ul> <li>Introduction</li> <li>Requirements</li> <li>Installation</li> <li>Dataset</li> <li>Configuration</li> <li>Adapting the model</li> <li>Conclusion</li> </ul>"},{"location":"examples/adapt/summarization/#introduction","title":"Introduction","text":"<p>In this example we will be finetuning a T5 large model for text summarization on the xsum dataset using QLoRA. QLoRA (Quantized LoRA) allows us to finetune a large model with a small memory requirement by freezing and quantizing the original model weights and only training the LoRA adapters. The adapters are then merged while saving the model. </p>"},{"location":"examples/adapt/summarization/#requirements","title":"Requirements","text":"<p>Before you begin, ensure that you have the following: - A GPU-enabled environment with CUDA support. - The Nyuntam repository cloned and set up as per the Installation Guide. - Docker</p>"},{"location":"examples/adapt/summarization/#installation","title":"Installation","text":""},{"location":"examples/adapt/summarization/#step-1-clone-the-nyuntam-repository","title":"Step 1: Clone the Nyuntam Repository","text":"<p>Clone the repository and navigate to the <code>nyuntam</code> directory:</p> <pre><code>$ git clone https://github.com/nyunAI/nyuntam.git\n$ cd nyuntam\n</code></pre>"},{"location":"examples/adapt/summarization/#step-2-set-up-the-workspace","title":"Step 2: Set Up the workspace","text":"<p>To setup the environment use the following command(s),</p> <pre><code>pip install git+https://github.com/nyunAI/nyunzero-cli.git\nnyun init {WORKSPACE_PATH} -e adapt\n</code></pre>"},{"location":"examples/adapt/summarization/#dataset","title":"Dataset","text":"<p>XSum Dataset is used for this example. The dataset is directly used from \ud83e\udd17hugginface. </p> <p>Sample :  | document                                                                 | summary                                                                 | id        | |--------------------------------------------------------------------------|--------------------------------------------------------------------------|-----------| | The full cost of damage in Newton Stewart, one of the areas worst affec\u2026 | Clean-up operations are continuing across the Scottish Borders and Dumf\u2026 | 35232142  | | A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 \u2026 | Two tourist buses have been destroyed by fire in a suspected arson atta\u2026 | 40143035  | | Ferrari appeared in a position to challenge until the final laps, when \u2026 | Lewis Hamilton stormed to pole position at the Bahrain Grand Prix ahead\u2026 | 35951548  | | John Edward Bates, formerly of Spalding, Lincolnshire, but now living i\u2026 | A former Lincolnshire Police officer carried out a series of sex attack\u2026 | 36266422  | | Patients and staff were evacuated from Cerahpasa hospital on Wednesday \u2026 | An armed man who locked himself into a room at a psychiatric hospital i\u2026 | 38826984  |</p>"},{"location":"examples/adapt/summarization/#configuration","title":"Configuration","text":"<p>The following YAML file is used for setting up the experiment : </p> <pre><code>JOB_SERVICE : Adapt\nJOB_ID: SUMM\nTASK : Seq2Seq_tasks\nsubtask : summarization\nmax_input_length : 512\nmax_target_length : 128\neval_metric : 'rouge' \ncuda_id : '0'\nOUTPUT_DIR : \"/user_data/jobs/Adapt/SUMM\"\nOVERWRITE_OUTPUT_DIR : False\nLOGGING_PATH: \"/user_data/logs/Adapt/SUMM\" \npacking : True\ndataset_text_field : 'text' \nmax_seq_length : 512\nflash_attention2 : false\nblocksize : 128\nSAVE_METHOD : 'state_dict'\n\n\n# DATASET_ARGS :\nDATASET : 'EdinburghNLP/xsum'\nDATA_VERSION : '1.0'\nMAX_TRAIN_SAMPLES : 1000\nMAX_EVAL_SAMPLES : 1000\nDATASET_CONFIG : {}\ninput_column : 'document'\ntarget_column : 'summary'\n\n# MODEL_ARGS :\nMODEL : \"t5\"\nMODEL_PATH :  'google-t5/t5-large'\nMODEL_VERSION : '1.0'\nCACHE_BOOL : False\n\n# TRAINING_ARGS :\nSEED : 56\nDO_TRAIN : True\nDO_EVAL : True\nNUM_WORKERS : 4\nBATCH_SIZE : 16\nEPOCHS : 1\nSTEPS : 1\nOPTIMIZER : 'adamw_torch'\nLR : 1e-4\nSCHEDULER_TYPE : 'linear'\nWEIGHT_DECAY : 0.0\nBETA1 : 0.9\nBETA2 : 0.999\nADAM_EPS : 1e-8 \nINTERVAL : 'epoch'\nINTERVAL_STEPS : 100\nNO_OF_CHECKPOINTS : 5\nFP16 : False\nRESUME_FROM_CHECKPOINT : False\nGRADIENT_ACCUMULATION_STEPS : 1\nGRADIENT_CHECKPOINTING : True\npredict_with_generate: True\ngeneration_max_length : 128\nREMOVE_UNUSED_COLUMNS : True\n\n# FINE_TUNING_ARGS :\nLAST_LAYER_TUNING : True\nFULL_FINE_TUNING : False\n\nPEFT_METHOD : 'LoRA'\n\n# LoRA_CONFIG :\nr : 16\nalpha : 8\ndropout : 0.1\npeft_type : 'LoRA'\ntarget_modules : \nfan_in_fan_out : False\ninit_lora_weights : True  \n\n# BNB_CONFIG :\nload_in_4bit : True\nbnb_4bit_compute_dtype : \"float16\"\nbnb_4bit_quant_type : \"nf4\"\nbnb_4bit_use_double_quant : False \n</code></pre>"},{"location":"examples/adapt/summarization/#adapting-the-model","title":"Adapting the model","text":"<p>With the yaml file configured, the adaptation process is initiated with the following command : </p> <pre><code>nyun run examples/adapt/summarization/config.yaml\n</code></pre> <p>Once the job starts, you will find the following directory structure in the <code>user_data</code> folder:</p> <pre><code>user_data/\n\u251c\u2500\u2500 jobs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Adapt\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 SUMM\n\u251c\u2500\u2500 logs\n \u00a0\u00a0 \u2514\u2500\u2500 Adapt\n  \u00a0     \u2514\u2500\u2500 SUMM\n  \u00a0         \u2514\u2500\u2500 log.log\n\n</code></pre> <p>The output model will be stored in <code>user_data/jobs/Adapt/SUMM/</code> directory and the final directory structure will be:</p> <pre><code>user_data/\n\u251c\u2500\u2500 jobs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Adapt\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 SUMM\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 merged_model_state_dict.pth\n\u251c\u2500\u2500 logs\n \u00a0\u00a0 \u2514\u2500\u2500 Adapt\n  \u00a0     \u2514\u2500\u2500 SUMM\n  \u00a0         \u2514\u2500\u2500 log.log\n\n</code></pre>"},{"location":"examples/adapt/summarization/#conclusion","title":"Conclusion","text":"<p>This guide has walked you through the process of adapting the T5-Large model using QLoRA for summarization on the xsum dataset. By employing QLoRA, we efficiently fine-tuned the model with a reduced memory footprint.. The configuration and setup steps were outlined, ensuring that even complex tasks like distributed training and low-rank adaptation are manageable. The final trained model and logs are organized in a clear directory structure, making it easy to retrieve and analyze results.</p> <p>Author: Panigrahi, Abhranta</p>"},{"location":"examples/adapt/text_generation/","title":"Finetuning Llama3-8b with QDoRA and FSDP","text":""},{"location":"examples/adapt/text_generation/#table-on-contents","title":"Table on Contents","text":"<ul> <li>Introduction</li> <li>Requirements</li> <li>Installation</li> <li>Dataset</li> <li>Configuration</li> <li>Adapting the model</li> <li>Conclusion</li> </ul>"},{"location":"examples/adapt/text_generation/#introduction","title":"Introduction","text":"<p>In this example we will be finetuning Llama3-8b with QDoRA and FSDP. We will be using the the Llama-1k dataset for this example but any dataset (properly formatted) can be used for this purpose. DoRA is a PEFT method which simillar to LoRA trains adapter by freezing and quantizing the original model weights.</p>"},{"location":"examples/adapt/text_generation/#requirements","title":"Requirements","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>A GPU-enabled environment with CUDA support.</li> <li>The Nyuntam repository cloned and set up as per the Installation Guide.</li> <li>Docker</li> </ul>"},{"location":"examples/adapt/text_generation/#installation","title":"Installation","text":""},{"location":"examples/adapt/text_generation/#step-1-clone-the-nyuntam-repository","title":"Step 1: Clone the Nyuntam Repository","text":"<p>Clone the repository and navigate to the <code>nyuntam</code> directory:</p> <pre><code>git clone https://github.com/nyunAI/nyuntam.git\ncd nyuntam\n</code></pre>"},{"location":"examples/adapt/text_generation/#step-2-set-up-the-workspace","title":"Step 2: Set Up the workspace","text":"<p>To setup the environment use the following command(s),</p> <pre><code>pip install git+https://github.com/nyunAI/nyunzero-cli.git\nnyun init {WORKSPACE_PATH} -e adapt\n</code></pre>"},{"location":"examples/adapt/text_generation/#dataset","title":"Dataset","text":"<p>For this example we are using the mlabonne/guanaco-llama2-1k dataset from \ud83e\udd17 huggingface. Other datasets might need proper formatting according to the model and the task.</p> Text [INST] Me gradu\u00e9 hace poco de la carrera de medicina \u00bfMe podr\u00edas aconsejar para conseguir r\u00e1pidamente un puesto de trabajo? [/INST] Esto vale tanto para m\u00e9dicos como para cualquier otra profesi\u00f3n tras finalizar los... [INST] \u0421\u0430\u043c\u044b\u0439 \u0432\u0435\u043b\u0438\u043a\u0438\u0439 \u0447\u0435\u043b\u043e\u0432\u0435\u043a \u0438\u0437 \u0432\u0441\u0435\u0445 \u0436\u0438\u0432\u0448\u0438\u0445 \u043d\u0430 \u043f\u043b\u0430\u043d\u0435\u0442\u0435? [/INST] \u0414\u043b\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u043d\u0443\u0436\u043d\u043e \u0432\u044b\u0431\u0440\u0430\u0442\u044c \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u0432\u0435\u043b\u0438\u0447\u0438\u044f \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430. \u041e\u0431\u044b\u0447\u043d\u043e \u0432\u0435\u043b\u0438\u043a\u0438\u043c \u043d\u0430\u0437\u044b\u0432\u0430\u044e\u0442 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0432\u043d\u0435\u0441 \u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0432\u043a\u043b\u0430\u0434 \u0432 \u043e\u0431\u0449\u0435\u0441\u0442\u0432\u043e \u0438\u043b\u0438 \u0441\u0438\u043b\u044c\u043d\u043e \u0432\u044b\u0434\u0435\u043b\u044f\u043b\u0441\u044f \u043d\u0430 \u0444\u043e\u043d\u0435... [INST] Compose a professional email with the following points: Me chinese cook 10 years Good good cook People love my food Can you hire me? [/INST] Thanks for the information. Unfortunately, your initial message... [INST] \u00bfQu\u00e9 juegos me recomendar\u00edas si me ha gustado Undertale? [/INST] Deltarune: Este es un juego creado por el mismo desarrollador de Undertale, Toby Fox. Al igual que Undertale, Delt"},{"location":"examples/adapt/text_generation/#configuration","title":"Configuration","text":"<p>The configuration yaml file is as follows:</p> <pre><code>#Training Arguments\nADAM_EPS: 1e-8\nBATCH_SIZE: 4\nBETA1: 0.9\nBETA2: 0.999\nCACHE_BOOL: false\nDO_EVAL: false\nDO_TRAIN: true\nEPOCHS: 0.01\nGRADIENT_ACCUMULATION_STEPS: 1\nGRADIENT_CHECKPOINTING: true\nGROUP_BY_LENGTH: true\nINTERVAL: steps\nINTERVAL_STEPS: 50\nLR: 0.0002\nNO_OF_CHECKPOINTS: 5\nNUM_WORKERS: 4\nOPTIMIZER: paged_adamw_32bit\nREMOVE_UNUSED_COLUMNS: true\nRESUME_FROM_CHECKPOINT: false\n\n#Dataset Arguments\nCUSTOM_DATASET_PATH: null\nDATASET: mlabonne/guanaco-llama2-1k\nDATASET_CONFIG: {}\nDATASET_FORMAT: null\nDATASET_ID: 27\nFP16: true\nSCHEDULER_TYPE: constant\nSEED: 56\nSTEPS: 100\nWEIGHT_DECAY: 0.001\nalpha: 16\nblocksize: 128\ndataset_text_field: text\nmax_seq_length: 512\npacking: true\n\n#Model Arguments\nLOCAL_MODEL_PATH: null\nMODEL: Llama-2\nMODEL_PATH: NousResearch/Llama-2-7b-hf\n\n#Basic Arguments\nID: 27\nJOB_ID: 100\nJOB_SERVICE: Adapt\nLOGGING_PATH: /user_data/logs/Adapt/100\nOUTPUT_DIR: /user_data/jobs/Adapt/100\nOVERWRITE_OUTPUT_DIR: false\nLibrary: Huggingface\nFSDP: true\nSAVE_METHOD: state_dict\nTASK: text_generation\nUSER_FOLDER: user_data\ncuda_id: '0,1,2,3'\nnum_nodes: 1\n\n#PEFT arguments\nFULL_FINE_TUNING: false\nLAST_LAYER_TUNING: true\nPEFT_METHOD: DoRA\ndropout: 0.1\nfan_in_fan_out: false\nflash_attention2: false\ninit_lora_weights: true\npeft_type: DoRA\nr: 2\ntarget_modules: null\nauto_select_modules: true\n\n#Quantization Arguments\nload_in_4bit: true\nbnb_4bit_compute_dtype: float16\nbnb_4bit_quant_type: nf4\nbnb_4bit_use_double_quant: false\n\n#FSDP configs\ncompute_environment: LOCAL_MACHINE\ndebug: true\ndistributed_type: FSDP\ndowncast_bf16: 'no'\nenable_cpu_affinity: false\nfsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\nfsdp_backward_prefetch: NO_PREFETCH\nfsdp_cpu_ram_efficient_loading: true\nfsdp_forward_prefetch: false\nfsdp_offload_params: false\nfsdp_sharding_strategy: FULL_SHARD\nfsdp_state_dict_type: SHARDED_STATE_DICT\nfsdp_sync_module_states: true\nfsdp_transformer_layer_cls_to_wrap: null\nfsdp_use_orig_params: true\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: 'no'\nnum_machines: 1\nnum_processes: 4\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: true\n</code></pre>"},{"location":"examples/adapt/text_generation/#adapting-the-model","title":"Adapting the model","text":"<p>With the yaml file configured, the adaptation process is initiated with the following command:</p> <pre><code>nyun run examples/adapt/text_generation/config.yaml\n</code></pre> <p>Once the job starts, you will find the following directory structure in the <code>user_data</code> folder:</p> <pre><code>user_data/\n\u251c\u2500\u2500 jobs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Adapt\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 100\n\u251c\u2500\u2500 logs\n \u00a0\u00a0 \u2514\u2500\u2500 Adapt\n  \u00a0     \u2514\u2500\u2500 100\n  \u00a0         \u2514\u2500\u2500 log.log\n\n</code></pre> <p>The output model will be stored in <code>user_data/jobs/Adapt/100/</code> directory and the final directory structure will be:</p> <pre><code>user_data/\n\u251c\u2500\u2500 jobs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Adapt\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 100\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 merged_model_state_dict.pth\n\u251c\u2500\u2500 logs\n \u00a0\u00a0 \u2514\u2500\u2500 Adapt\n  \u00a0     \u2514\u2500\u2500 100\n  \u00a0         \u2514\u2500\u2500 log.log\n\n</code></pre>"},{"location":"examples/adapt/text_generation/#conclusion","title":"Conclusion","text":"<p>This guide has walked you through the process of adapting the Llama3-8b model using QDoRA for text generation. By employing QDoRA, we efficiently fine-tuned the model with a reduced memory footprint. The configuration and setup steps were outlined, ensuring that even complex tasks like distributed training and low-rank adaptation are manageable. The final trained model and logs are organized in a clear directory structure, making it easy to retrieve and analyze results.</p> <p>Author: Panigrahi, Abhranta</p>"},{"location":"examples/text-generation/aqlm_quantization/readme/","title":"Maximising math performance for extreme compressions: 2-bit Llama3-8b (w2a16)","text":""},{"location":"examples/text-generation/aqlm_quantization/readme/#overview","title":"Overview","text":"<p>This guide provides a detailed walkthrough on maximizing the performance of a highly compressed Llama3-8b model using 2-bit weights and 16-bit activations. We will apply the Additive Quantization for Large Models (AQLM) technique to compress and optimize the Llama3-8b model, drastically reducing its memory footprint while maintaining performance.</p>"},{"location":"examples/text-generation/aqlm_quantization/readme/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Requirements</li> <li>Installation</li> <li>Configuration</li> <li>Running the Quantization</li> <li>Performance Evaluation</li> <li>Conclusion</li> </ul>"},{"location":"examples/text-generation/aqlm_quantization/readme/#introduction","title":"Introduction","text":"<p>In this guide, we will try and maximise the Grade School Math abilities of an extremely compressed Llama3-8b model.</p>"},{"location":"examples/text-generation/aqlm_quantization/readme/#requirements","title":"Requirements","text":"<p>Before starting, ensure that you have the following:</p> <ul> <li>A GPU-enabled environment with CUDA support.</li> <li>The nyuntam repository cloned and set up as per the Installation Guide.</li> </ul>"},{"location":"examples/text-generation/aqlm_quantization/readme/#installation","title":"Installation","text":""},{"location":"examples/text-generation/aqlm_quantization/readme/#step-1-clone-the-nyuntam-repository","title":"Step 1: Clone the Nyuntam Repository","text":"<p>Clone the repository and navigate to the <code>nyuntam</code> directory:</p> <pre><code>git clone https://github.com/nyunAI/nyuntam.git\ncd nyuntam\ngit submodule update --init text_generation\ncd text_generation\ngit submodule update --init quantization/aqlm/AQLM\ncd ..\n</code></pre>"},{"location":"examples/text-generation/aqlm_quantization/readme/#step-2-set-up-the-environment","title":"Step 2: Set Up the Environment","text":"<p>Create and activate an environment for the AQLM quantization:</p> <pre><code>conda create -n aqlm_quantization python=3.10 # or use virtualenv if preferred\nconda activate aqlm_quantization\n</code></pre> <p>Install the required dependencies:</p> <pre><code>pip install torch==2.3.0 # (adjust version if needed)\npip install -r text_generation/quantization/aqlm/requirements.txt\n</code></pre>"},{"location":"examples/text-generation/aqlm_quantization/readme/#experimentations","title":"Experimentations","text":""},{"location":"examples/text-generation/aqlm_quantization/readme/#step-1-sft-iterative-dpo","title":"Step 1: SFT + Iterative DPO","text":"<p>We first apply SFT + Iterative DPO to the model to boost upfront the downstream task performance. For a quicker reproducibility, we use the llama3 checkpoints provided by RLHFlow - <code>RLHFlow/LLaMA3-iterative-DPO-final</code> for this experiment.</p>"},{"location":"examples/text-generation/aqlm_quantization/readme/#step-2-aqlm-quantization","title":"Step 2: AQLM Quantization","text":"<p>Dataset: We use the <code>openai/gsm8k</code> dataset for calibration and fine-tuning of the quantized model. Use the following script to create the dataset:</p> <pre><code>python examples/text-generation/aqlm_quantization/create_dataset.py\n</code></pre> <p>Next, we quantize and finetune the model.</p>"},{"location":"examples/text-generation/aqlm_quantization/readme/#configuration","title":"Configuration","text":"<p>Prepare the YAML configuration file specific to AQLM quantization. Use the following template as a starting point:</p> <pre><code># aqlm_quantization.yaml\n\n# Model configuration\nMODEL: \"RLHFlow/LLaMA3-iterative-DPO-final\" # this can be meta-llama/Meta-Llama-3.1-8B or any other model from huggingface\n\n# Data configuration\nDATASET_NAME: \"togethercomputer/RedPajama-Data-1T-Sample\"\nTEXT_COLUMN: \"text\"                     \nSPLIT: \"train\"\n\n# Data configuration (if finetuning on gsm8k)\n# DATASET_NAME: \"gsm8k_restructured\"\n# DATA_PATH: \"user_data/datasets/gsm8k_restructured\"\n# TEXT_COLUMN: \"text\"                     \n# SPLIT: \"train\"\n\nDATASET_SUBNAME: \"\"\nFORMAT_STRING:\n\n# Quantization configuration\n\nllm:\n  AQLM:\n    # Quantization parameters\n    save_intermediate_results: true\n    dtype: \"float16\"\n    overwrite: false\n\n    # ...other params\n\n# Job configuration\nCUDA_ID: \"0,1,2,3\"\nALGORITHM: \"AQLM\"\nJOB_SERVICE: \"Kompress\"\nUSER_FOLDER: \"user_data\"\nJOB_ID: \"aqlm_quantization\"\nCACHE_PATH: \"user_data/.cache\"\nJOB_PATH: \"user_data/jobs/aqlm_quantization\"\nLOGGING_PATH: \"user_data/logs/aqlm_quantization\"\nALGO_TYPE: \"llm\"\nTASK: \"llm\"\n</code></pre>"},{"location":"examples/text-generation/aqlm_quantization/readme/#running-the-quantization","title":"Running the Quantization","text":"<pre><code>python main.py --yaml_path examples/text-generation/aqlm_quantization/config.yaml\n</code></pre>"},{"location":"examples/text-generation/aqlm_quantization/readme/#running-the-finetuning","title":"Running the Finetuning","text":"<p>With your YAML file configured, initiate the quantization process by running:</p> <pre><code>torchrun --nproc-per-node=4 main.py --yaml_path examples/text-generation/aqlm_quantization/config.yaml\n</code></pre> <p>Monitor the process to ensure the quantization completes successfully.</p> <p>Once the job starts, the following directory structure will be created in the <code>user_data</code> folder:</p> <pre><code>user_data/\n\u251c\u2500\u2500 datasets\n\u2502   \u251c\u2500\u2500 gsm8k_restructured\n\u2502   \u2514\u2500\u2500 togethercomputer\n\u2502       \u2514\u2500\u2500 RedPajama-Data-1T-Sample\n\u251c\u2500\u2500 jobs\n\u2502   \u2514\u2500\u2500 Kompress\n\u2502       \u2514\u2500\u2500 aqlm_quantization\n\u2502           \u2514\u2500\u2500 tmp\n\u2502               \u251c\u2500\u2500 caliberation\n\u2502               \u2514\u2500\u2500 tokenized_dataset\n|               ...\n\u251c\u2500\u2500 logs\n\u2502   \u2514\u2500\u2500 aqlm_quantization\n\u2514\u2500\u2500 models\n    \u2514\u2500\u2500 RLHFlow\n        \u2514\u2500\u2500 LLaMA3-iterative-DPO-final\n</code></pre> <p>The quantized model will be saved in the <code>user_data/jobs/Kompress/aqlm_quantization</code> directory:</p> <pre><code>user_data/\n\u2514\u2500\u2500 jobs\n    \u2514\u2500\u2500 Kompress\n        \u2514\u2500\u2500 aqlm_quantization\n            ...\n</code></pre>"},{"location":"examples/text-generation/aqlm_quantization/readme/#performance-evaluation","title":"Performance Evaluation","text":"<p>After quantization, evaluate the performance of the quantized model using the provided evaluation script:</p> <pre><code>pip install lm-eval\n\n## ===== GSM8K Evaluation =====\n\n# baseline gsm8k 5 shot evaluation\naccelerate launch --no-python lm_eval --model hf \\\n  --model_args pretrained=meta-llama/Meta-Llama-3-8B-Instruct,cache_dir=user_data/.cache \\\n  --tasks gsm8k \\\n  --num_fewshot 5 \\\n  --batch_size \"auto\" \\\n  --output_path user_data/evals/meta-llama_3.1-8b/base/gsm8k/\n\n# Llama3* gsm8k 5 shot evaluation\naccelerate launch --no-python lm_eval --model hf \\\n  --model_args pretrained=RLHFlow/LLaMA3-iterative-DPO-final,cache_dir=user_data/.cache \\\n  --tasks gsm8k \\\n  --num_fewshot 5 \\\n  --batch_size \"auto\" \\\n  --output_path user_data/evals/meta-llama_3.1-8b/sft+iterative_dpo/gsm8k/\n\n# Llama3Q gsm8k 5 shot evaluation\npython examples/text-generation/aqlm_quantization/evaluate.py \\\n  --base_model \"meta-llama/Meta-Llama-3-8B-Instruct\" \\\n  --quantized_model \"user_data/jobs/Kompress/aqlm_quantization/tmp/caliberation/\" \\\n  --tasks \"gsm8k:5\" \\\n  --results \"user_data/evals/meta-llama_3.1-8b/Llama3Q\" \\\n  --cache_dir \"user_data/.cache\"\n\n# Llama3Q PV Tuned gsm8k 5 shot evaluation\npython examples/text-generation/aqlm_quantization/evaluate.py \\\n  --base_model \"meta-llama/Meta-Llama-3-8B-Instruct\" \\\n  --quantized_model \"user_data/jobs/Kompress/aqlm_quantization/tmp/converted/\" \\\n  --tasks \"gsm8k:5\" \\\n  --results \"user_data/evals/meta-llama_3.1-8b/Llama3Q_PV_Tuned\" \\\n  --cache_dir \"user_data/.cache\"\n\n## ===== Preplexity Evaluation =====\n\n# Llama3* perplexity evaluation\npython examples/text-generation/aqlm_quantization/evaluate.py \\\n  --base_model \"RLHFlow/LLaMA3-iterative-DPO-final\" \\\n  --tasks \"gptq_wikitext:0\" \\\n  --results \"user_data/evals/meta-llama_3.1-8b/Llama3*\" \\\n  --cache_dir \"user_data/.cache\"\n\n# baseline &amp; Llama3Q perplexity evaluation\npython examples/text-generation/aqlm_quantization/evaluate.py \\\n  --base_model \"meta-llama/Meta-Llama-3-8B-Instruct\" \\\n  --quantized_model \"user_data/jobs/Kompress/aqlm_quantization/tmp/caliberation/\" \\\n  --tasks \"gptq_wikitext:0\" \\\n  --results \"user_data/evals/meta-llama_3.1-8b\" \\\n  --cache_dir \"user_data/.cache\"\n\n# Llama3Q PV Tuned perplexity evaluation\npython examples/text-generation/aqlm_quantization/evaluate.py \\\n  --base_model \"meta-llama/Meta-Llama-3-8B-Instruct\" \\\n  --quantized_model \"user_data/jobs/Kompress/aqlm_quantization/tmp/converted/\" \\\n  --tasks \"gptq_wikitext:0\" \\\n  --results \"user_data/evals/meta-llama_3.1-8b/Llama3Q_PV_Tuned\" \\\n  --cache_dir \"user_data/.cache\"\n</code></pre> <p>Compare the results with the original model to assess the impact of quantization on accuracy and inference speed.</p> Llama3(Llama3-8b) Llama3*(SFT + Iterative DPO Llama3) Llama3Q PV Tuned(Quantized + PV Tuned Llama3*) GSM8K (5 shot) 50.9 78.99 58.9"},{"location":"examples/text-generation/aqlm_quantization/readme/#conclusion","title":"Conclusion","text":"<p>From the results, we can see that the Llama3Q PV Tuned model achieves a GSM8K score of 58.9, which is a significant improvement over the baseline Llama3-8b model. The model has been compressed to 2-bit weights and 16-bit activations, reducing its memory footprint while maintaining performance.</p> <p>Author: Kushwaha, Shubham</p>"},{"location":"examples/text-generation/aqlm_quantization/readme/#additional-examples","title":"Additional Examples","text":"<ul> <li>Efficient 4-bit Quantization (w4a16) of Llama3.1-8b for Optimized Text Generation</li> <li>Llama3.1 70B: 0.5x the cost &amp; size</li> <li>Achieving Up to 2.5x TensorRTLLM Speedups: Efficient 4-8-4 Quantization (w4a8kv4) of Llama3.1-8b</li> <li>Accelerating a 4-bit Quantised Llama Model</li> </ul>"},{"location":"examples/text-generation/awq_quantization/readme/","title":"Efficient 4-bit Quantization (w4a16) of Llama3.1-8b for Optimized Text Generation","text":""},{"location":"examples/text-generation/awq_quantization/readme/#overview","title":"Overview","text":"<p>This guide provides a walkthrough of applying AWQ (Activation-aware Weight Quantization) to compress and accelerate the Llama3.1-8b model using 4-bit weights and 16-bit activations. AWQ allows for significant reduction in model size and computational requirements without sacrificing performance, making it an excellent choice for deploying large language models in resource-constrained environments.</p>"},{"location":"examples/text-generation/awq_quantization/readme/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Requirements</li> <li>Installation</li> <li>Configuration</li> <li>Running the Quantization</li> <li>Performance Evaluation</li> <li>Conclusion</li> </ul>"},{"location":"examples/text-generation/awq_quantization/readme/#introduction","title":"Introduction","text":"<p>In this example, we'll be utilizing the AWQ technique to quantize the Llama3.1-8b model, aiming to reduce its memory footprint and improve inference speed. AWQ's flexibility in precision makes it a versatile tool for adapting large models to various hardware configurations.</p>"},{"location":"examples/text-generation/awq_quantization/readme/#requirements","title":"Requirements","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>A GPU-enabled environment with CUDA support.</li> <li>The Nyuntam repository cloned and set up as per the Installation Guide.</li> </ul>"},{"location":"examples/text-generation/awq_quantization/readme/#installation","title":"Installation","text":""},{"location":"examples/text-generation/awq_quantization/readme/#step-1-clone-the-nyuntam-repository","title":"Step 1: Clone the Nyuntam Repository","text":"<p>Clone the repository and navigate to the <code>nyuntam</code> directory:</p> <pre><code>git clone https://github.com/nyunAI/nyuntam.git\ncd nyuntam\n</code></pre>"},{"location":"examples/text-generation/awq_quantization/readme/#step-2-set-up-the-workspace","title":"Step 2: Set Up the workspace","text":"<p>Create and activate an environment for the AWQ quantization example:</p> <pre><code>conda create -n awq_quantization python=3.10 # or use virtualenv if preferred\nconda activate awq_quantization\n</code></pre> <p>Install the required dependencies:</p> <pre><code>pip install torch==2.3.0 # (any other version as suitable)\npip install -r text_generation/quantization/autoawq/requirements.txt\n</code></pre>"},{"location":"examples/text-generation/awq_quantization/readme/#configuration","title":"Configuration","text":"<p>Prepare the YAML configuration file specific to AWQ quantization. Use the following template as a starting point:</p> <pre><code># awq_quantization.yaml\n\n# Model configuration\nMODEL: \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\n# Data configuration\nDATASET_NAME: \"wikitext\"\nDATASET_SUBNAME: \"wikitext-2-raw-v1\"\nTEXT_COLUMN: \"text\"                     \nSPLIT: \"train\"\n\nDATA_PATH:\nFORMAT_STRING:\n\n# Quantization configuration\nllm:\n  AutoAWQ:\n    ZERO_POINT: True                    # zero point quantization\n    W_BIT: 4                            # weight bitwidth\n    Q_GROUP_SIZE: 128                   # group size for quantization [default: 128, 64, 32]\n    VERSION: \"GEMV\"                     # quantization version (GEMM or GEMV)\n\n# Job configuration\nCUDA_ID: \"0\"\nALGORITHM: \"AutoAWQ\"\nJOB_SERVICE: \"Kompress\"\nUSER_FOLDER: \"user_data\"\nJOB_ID: \"awq_quantization\"\nCACHE_PATH: \"user_data/.cache\"\nJOB_PATH: \"user_data/jobs/awq_quantization\"\nLOGGING_PATH: \"user_data/logs/awq_quantization\"\nALGO_TYPE: \"llm\"\nTASK: \"llm\"\n</code></pre>"},{"location":"examples/text-generation/awq_quantization/readme/#running-the-quantization","title":"Running the Quantization","text":"<p>With your YAML file configured, initiate the quantization process by running:</p> <pre><code>python main.py --yaml_path examples/text-generation/awq_quantization/config.yaml\n</code></pre> <p>Monitor the process to ensure that the quantization completes successfully.</p> <p>Once the job starts, you'll find the following directory structure in the <code>user_data</code> folder:</p> <pre><code>user_data/\n\u251c\u2500\u2500 datasets\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 wikitext\n\u251c\u2500\u2500 jobs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Kompress\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 awq_quantization\n\u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 awq_quantization\n\u2514\u2500\u2500 models\n    \u2514\u2500\u2500 meta-llama\n        \u2514\u2500\u2500 Meta-Llama-3.1-8B-Instruct\n</code></pre> <p>The output model will be saved in the <code>user_data/jobs/Kompress/awq_quantization</code> directory.</p> <pre><code>user_data/\n\u2514\u2500\u2500 jobs\n    \u2514\u2500\u2500 Kompress\n        \u2514\u2500\u2500 awq_quantization\n            \u251c\u2500\u2500 config.json\n            \u251c\u2500\u2500 generation_config.json\n            \u251c\u2500\u2500 model.safetensors\n            \u251c\u2500\u2500 special_tokens_map.json\n            \u251c\u2500\u2500 tokenizer.json\n            \u2514\u2500\u2500 tokenizer_config.json\n</code></pre>"},{"location":"examples/text-generation/awq_quantization/readme/#performance-evaluation","title":"Performance Evaluation","text":"<p>After the quantization process, evaluate the performance of the quantized model using the evaluation script provided.</p> <pre><code>pip install lm-eval git+https://github.com/PanQiWei/AutoGPTQ.git\n\n# wikitext perplexity evaluation\npython examples/text-generation/awq_quantization/evaluate.py \\\n  --base_model \"meta-llama/Meta-Llama-3.1-8B-Instruct\" \\\n  --quantized_model \"user_data/jobs/Kompress/awq_quantization\" \\\n  --tasks \"gptq_wikitext:0\" \\\n  --results \"user_data/evals/meta-llama_3.1-8b\" \\\n  --cache_dir \"user_data/.cache\"\n\n# baseline arc_challenge 25 shot evaluation\naccelerate launch --no-python lm_eval --model hf \\\n  --model_args pretrained=meta-llama/Meta-Llama-3.1-8B-Instruct,cache_dir=user_data/.cache \\\n  --tasks arc_challenge \\\n  --num_fewshot 25 \\\n  --batch_size 16 \\\n  --output_path user_data/evals/meta-llama_3.1-8b/base/arc_challenge/\n\n# quantized arc_challenge 25 shot evaluation\naccelerate launch --no-python lm_eval --model hf \\\n  --model_args pretrained=user_data/jobs/Kompress/awq_quantization,cache_dir=user_data/.cache \\\n  --tasks arc_challenge \\\n  --num_fewshot 25 \\\n  --batch_size 16 \\\n  --output_path user_data/evals/meta-llama_3.1-8b/quantized/arc_challenge/\n</code></pre> <p>Compare the results with the original model to assess the impact of quantization on accuracy and inference speed.</p> Model Task Metric Baseline Quantized Impact Llama3.1-8b Perplexity (wikitext, gptq) Perplexity \u2193 7.32 7.69 +0.37 Llama3.1-8b ARC Challenge (25 shot) Accuracy \u2191 60.66 59.73 -0.93 Llama3.1-8b Size (in GB) Model Size \u2193 15.0 05.4 -09.60"},{"location":"examples/text-generation/awq_quantization/readme/#conclusion","title":"Conclusion","text":"<p>AWQ quantization offers a practical approach to optimizing large models like meta-llama/Meta-Llama-3.1-8B-Instruct for deployment in limited-resource environments. By following this guide, you should now have a quantized model that balances performance and efficiency.</p> <p>Author: Kushwaha, Shubham</p>"},{"location":"examples/text-generation/awq_quantization/readme/#additional-examples","title":"Additional Examples","text":"<ul> <li>Maximising math performance for extreme compressions: 2-bit Llama3-8b (w2a16)</li> <li>Llama3.1 70B: 0.5x the cost &amp; size</li> <li>Achieving Up to 2.5x TensorRTLLM Speedups: Efficient 4-8-4 Quantization (w4a8kv4) of Llama3.1-8b</li> <li>Accelerating a 4-bit Quantised Llama Model</li> </ul>"},{"location":"examples/text-generation/flap_pruning/readme/","title":"Llama3.1-60B-Instruct: Removing 10B parameters (15%) with minimal performance loss and no retraining.","text":""},{"location":"examples/text-generation/flap_pruning/readme/#overview","title":"Overview","text":"<p>This guide provides a walkthrough of applying FLAP (Fluctuation-based Adaptive Structured Pruning) to compress and accelerate the Llama3.1-70b-instruct model. FLAP allows for significant reduction in model size and computational requirements without sacrificing performance. Unlike traditional pruning techniques, FLAP requires no retraining and adapts the pruning ratio across different modules and layers, offering an efficient and effective approach for deploying large language models in resource-constrained environments.</p>"},{"location":"examples/text-generation/flap_pruning/readme/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Requirements</li> <li>Installation</li> <li>Configuration</li> <li>Running the Pruning</li> <li>Performance Evaluation</li> </ul>"},{"location":"examples/text-generation/flap_pruning/readme/#introduction","title":"Introduction","text":"<p>In this example, we'll be utilizing the FLAP technique to prune the Llama3.1-70b model, aiming to reduce its memory footprint and improve inference speed. FLAP's adaptive compression structure and no-training-required approach make it a versatile tool for adapting large models to various hardware configurations.</p>"},{"location":"examples/text-generation/flap_pruning/readme/#requirements","title":"Requirements","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>A GPU-enabled environment with CUDA support.</li> <li>The Nyuntam repository cloned and set up as per the Installation Guide.</li> </ul>"},{"location":"examples/text-generation/flap_pruning/readme/#installation","title":"Installation","text":""},{"location":"examples/text-generation/flap_pruning/readme/#step-1-clone-the-nyuntam-repository","title":"Step 1: Clone the Nyuntam Repository","text":"<p>Clone the repository and navigate to the <code>nyuntam</code> directory:</p> <pre><code>git clone https://github.com/nyunAI/nyuntam.git\ncd nyuntam\n</code></pre>"},{"location":"examples/text-generation/flap_pruning/readme/#step-2-set-up-the-workspace","title":"Step 2: Set Up the Workspace","text":"<p>Create and activate an environment for the FLAP pruning example:</p> <pre><code>conda create -n flap_pruning python=3.10 # or use virtualenv if preferred\nconda activate flap_pruning\n</code></pre> <p>Install the required dependencies:</p> <pre><code>pip install -r text_generation/pruning/flap/requirements.txt\n</code></pre>"},{"location":"examples/text-generation/flap_pruning/readme/#configuration","title":"Configuration","text":"<p>Prepare the YAML configuration file specific to FLAP pruning. Use the following template as a starting point:</p> <pre><code># flap_pruning.yaml\n\n# Model configuration\nMODEL: \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n\n# Data configuration\nDATASET_NAME: \"wikitext\"\nDATASET_SUBNAME: \"wikitext-2-raw-v1\"\nTEXT_COLUMN: \"text\"                     \nSPLIT: \"train\"\n\nDATA_PATH:\nFORMAT_STRING:\n\n# Quantization configuration\nllm:\n  FlapPruner:\n    dtype: \"float16\"\n    metrics: \"WIFV\"\n    pruning_ratio: 0.5\n    remove_heads: -1\n    start_pruning_layer_idx: 56\n    structure: \"AL-AM\"\n\n    to_finetune: False\n\n# Job configuration\nCUDA_ID: \"0,1,2,3\"\nALGORITHM: \"FlapPruner\"\nJOB_SERVICE: \"Kompress\"\nUSER_FOLDER: \"user_data\"\nJOB_ID: \"flap_pruning\"\nCACHE_PATH: \"user_data/.cache\"\nJOB_PATH: \"user_data/jobs/flap_pruning\"\nLOGGING_PATH: \"user_data/logs/flap_pruning\"\nALGO_TYPE: \"llm\"\nTASK: \"llm\"\n</code></pre>"},{"location":"examples/text-generation/flap_pruning/readme/#running-the-pruning","title":"Running the Pruning","text":"<p>With your YAML file configured, initiate the pruning process by running:</p> <pre><code>python main.py --yaml_path examples/text-generation/flap_pruning/config.yaml\n</code></pre> <p>Monitor the process to ensure that the pruning completes successfully.</p> <p>Once the job starts, you'll find the following directory structure in the <code>user_data</code> folder:</p> <pre><code>user_data/\n\u251c\u2500\u2500 datasets\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 wikitext\n\u251c\u2500\u2500 jobs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Kompress\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 flap_pruning\n\u251c\u2500\u2500 logs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 flap_pruning\n\u2514\u2500\u2500 models\n    \u2514\u2500\u2500 meta-llama\n        \u2514\u2500\u2500 Meta-Llama-3.1-70B-Instruct\n</code></pre> <p>The pruned model will be saved in the <code>user_data/jobs/Kompress/flap_pruning</code> directory.</p> <pre><code>user_data/\n\u2514\u2500\u2500 jobs\n    \u2514\u2500\u2500 Kompress\n        \u2514\u2500\u2500 flap_pruning\n            \u251c\u2500\u2500 config.json\n            \u251c\u2500\u2500 generation_config.json\n            \u251c\u2500\u2500 model-00001-of-00019.safetensors\n            \u251c\u2500\u2500 model-00002-of-00019.safetensors\n            \u251c\u2500\u2500 model-00003-of-00019.safetensors\n            \u251c\u2500\u2500 model-00004-of-00019.safetensors\n            \u251c\u2500\u2500 model-00005-of-00019.safetensors\n            \u251c\u2500\u2500 model-00006-of-00019.safetensors\n            \u251c\u2500\u2500 model-00007-of-00019.safetensors\n            \u251c\u2500\u2500 model-00008-of-00019.safetensors\n            \u251c\u2500\u2500 model-00009-of-00019.safetensors\n            \u251c\u2500\u2500 model-00010-of-00019.safetensors\n            \u251c\u2500\u2500 model-00011-of-00019.safetensors\n            \u251c\u2500\u2500 model-00012-of-00019.safetensors\n            \u251c\u2500\u2500 model-00013-of-00019.safetensors\n            \u251c\u2500\u2500 model-00014-of-00019.safetensors\n            \u251c\u2500\u2500 model-00015-of-00019.safetensors\n            \u251c\u2500\u2500 model-00016-of-00019.safetensors\n            \u251c\u2500\u2500 model-00017-of-00019.safetensors\n            \u251c\u2500\u2500 model-00018-of-00019.safetensors\n            \u251c\u2500\u2500 model-00019-of-00019.safetensors\n            \u251c\u2500\u2500 model.safetensors.index.json\n            \u251c\u2500\u2500 prune_config.json\n            \u251c\u2500\u2500 special_tokens_map.json\n            \u251c\u2500\u2500 tokenizer.json\n            \u2514\u2500\u2500 tokenizer_config.json\n</code></pre>"},{"location":"examples/text-generation/flap_pruning/readme/#performance-evaluation","title":"Performance Evaluation","text":"<p>After the pruning process, evaluate the performance of the pruned model using the evaluation script provided.</p> <pre><code>pip install lm-eval git+https://github.com/PanQiWei/AutoGPTQ.git\n\npython examples/text-generation/flap_pruning/evaluate.py \\\n  --base_model \"meta-llama/Meta-Llama-3.1-70B-Instruct\" \\\n  --pruned_model \"user_data/jobs/Kompress/flap_pruning/\" \\\n  --tasks \"mmlu:5,gptq_wikitext:0\" \\\n  --results \"user_data/evals/meta-llama_3.1-70b\" \\\n  --cache_dir \"user_data/.cache\"\n\n</code></pre> <p>Compare the results with the original model to assess the impact of pruning on accuracy and inference speed.</p> Model Task Metric Baseline Pruned Impact Llama3.1-70b-Instruct MMLU (5 shot) Accuracy \u2191 83.6 82.31 -1.29 Llama3.1-70b-Instruct Num. parameters Count (in billions, B) \u2193 70.56 60.16 -10.4 (14.74%) <p>The results show that the pruned model retains a high level of accuracy while reducing the number of parameters by ~15%.</p> <p>Author: Kushwaha, Shubham</p>"},{"location":"examples/text-generation/flap_pruning/readme/#additional-examples","title":"Additional Examples","text":"<ul> <li>Maximising math performance for extreme compressions: 2-bit Llama3-8b (w2a16)</li> <li>Efficient 4-bit Quantization (w4a16) of Llama3.1-8b for Optimized Text Generation</li> <li>Achieving Up to 2.5x TensorRTLLM Speedups: Efficient 4-8-4 Quantization (w4a8kv4) of Llama3.1-8b</li> <li>Accelerating a 4-bit Quantised Llama Model</li> </ul>"},{"location":"examples/text-generation/lmquant_quantization/readme/","title":"Achieving Up to 2.5x TensorRTLLM Speedups: Efficient 4-8-4 Quantization (w4a8kv4) of Llama3.1-8b","text":""},{"location":"examples/text-generation/lmquant_quantization/readme/#overview","title":"Overview","text":"<p>This guide provides a detailed walkthrough of applying LMQuant using the QoQ algorithm (quattuor-octo-quattuor) to quantize the Llama3.1-8b model. By using 4-bit weights, 8-bit activations, and 4-bit key-value cache (W4A8KV4), LMQuant aims to significantly reduce model size while maintaining high performance and efficient inference speed. This process is particularly beneficial for deploying large language models in environments with limited resources.</p>"},{"location":"examples/text-generation/lmquant_quantization/readme/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Requirements</li> <li>Installation</li> <li>Configuration</li> <li>Running the Quantization</li> <li>Performance Evaluation</li> <li>Conclusion</li> </ul>"},{"location":"examples/text-generation/lmquant_quantization/readme/#introduction","title":"Introduction","text":"<p>In this example, we will use the QoQ algorithm provided by LMQuant to quantize the Llama3.1-8b model. QoQ (quattuor-octo-quattuor) utilizes W4A8KV4 quantization, a method that effectively compresses the model without sacrificing significant performance, making it suitable for deployment on both edge devices and large-scale servers.</p>"},{"location":"examples/text-generation/lmquant_quantization/readme/#requirements","title":"Requirements","text":"<p>Before starting, ensure that you have the following:</p> <ul> <li>A GPU-enabled environment with CUDA support.</li> <li>The LMQuant repository cloned and set up as described in the Installation Guide.</li> </ul>"},{"location":"examples/text-generation/lmquant_quantization/readme/#installation","title":"Installation","text":""},{"location":"examples/text-generation/lmquant_quantization/readme/#step-1-clone-the-nyuntam-repository","title":"Step 1: Clone the Nyuntam Repository","text":"<p>Clone the repository and navigate to the <code>nyuntam</code> directory:</p> <pre><code>git clone https://github.com/nyunAI/nyuntam.git\ncd nyuntam\n</code></pre>"},{"location":"examples/text-generation/lmquant_quantization/readme/#step-2-set-up-the-workspace","title":"Step 2: Set Up the workspace","text":"<p>Create and activate an environment for the AWQ quantization example:</p> <pre><code>conda create -n lmquant_quantization python=3.10 # or use virtualenv if preferred\nconda activate lmquant_quantization\n</code></pre> <p>Install the required dependencies:</p> <pre><code>pip install torch==2.3.0 # (any other version as suitable)\npip install -r text_generation/engines/mit_han_lab_qserve/requirements.txt text_generation/engines/mit_han_lab_qserve/QServe/kernels # for QServe\npip install -r text_generation/quantization/mit_han_lab_lmquant/requirements.txt\n</code></pre>"},{"location":"examples/text-generation/lmquant_quantization/readme/#configuration","title":"Configuration","text":"<p>Prepare the YAML configuration file specific to the QoQ quantization. Use the following template as a starting point:</p> <pre><code># lmquant_quantization.yaml\n\n# Model configuration\nMODEL: \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\n# Data configuration\nDATASET_NAME: \"wikitext\"\nDATASET_SUBNAME: \"wikitext-2-raw-v1\"\nTEXT_COLUMN: \"text\"                     \nSPLIT: \"train\"\n\nDATA_PATH:\nFORMAT_STRING:\n\n# Quantization configuration\nllm:\n  LMQuant:\n    # Quantization parameters\n    save_model: True\n    keep_scales: True\n    loads_with_qserve: False\n    dtype: float32\n\n    quant_type: \"gchn\"\n    quant.develop_dtype: torch.float32\n    quant.smooth.xw.alpha: 0.05 \n    quant.smooth.xw.beta: 0.95 \n    quant.smooth.yx.strategy: GridSearch \n    quant.smooth.yx.beta: \" -2\"\n\n    quant.wgts.calib_range.outputs_device: cpu\n    quant.reorder.outputs_device: cpu\n    quant.smooth.xw.outputs_device: cpu\n    quant.smooth.yx.outputs_device: cpu\n\n    # Nested dictionary for quantization parameters\n    eval.tasks: [\"wikitext\", \"arc_challenge\"]\n    eval.max_seq_length: 4096\n    eval.evaluator: \"lm_eval\"\n\n\n# Job configuration\nCUDA_ID: \"0,1,2,3\"\nALGORITHM: \"LMQuant\"\nJOB_SERVICE: \"Kompress\"\nUSER_FOLDER: \"user_data\"\nJOB_ID: \"lmquant_quantization\"\nCACHE_PATH: \"user_data/.cache\"\nJOB_PATH: \"user_data/jobs/lmquant_quantization\"\nLOGGING_PATH: \"user_data/logs/lmquant_quantization\"\nALGO_TYPE: \"llm\"\nTASK: \"llm\"\n</code></pre>"},{"location":"examples/text-generation/lmquant_quantization/readme/#running-the-quantization","title":"Running the Quantization","text":"<p>With your YAML file configured, initiate the quantization process by running:</p> <pre><code>python main.py --yaml_path examples/text-generation/lmquant_quantization/config.yaml\n</code></pre> <p>Monitor the process to ensure that the quantization completes successfully.</p> <p>The output model will be saved in the <code>user_data/jobs/qoq_quantization</code> directory.</p> <pre><code>user_data/\n\u2514\u2500\u2500 jobs\n    \u2514\u2500\u2500 QServe\n        \u2514\u2500\u2500 qoq_quantization\n            \u251c\u2500\u2500 config.json\n            \u251c\u2500\u2500 generation_config.json\n            \u251c\u2500\u2500 model.safetensors\n            \u251c\u2500\u2500 special_tokens_map.json\n            \u251c\u2500\u2500 tokenizer.json\n            \u2514\u2500\u2500 tokenizer_config.json\n</code></pre>"},{"location":"examples/text-generation/lmquant_quantization/readme/#performance-evaluation","title":"Performance Evaluation","text":"<p>After quantization, evaluate the performance of the quantized model using the evaluation script provided.</p> <pre><code>pip install lm-eval git+https://github.com/PanQiWei/AutoGPTQ.git\n\n# wikitext perplexity evaluation\npython examples/text-generation/qoq_quantization/evaluate.py \\\n  --base_model \"meta-llama/Meta-Llama-3.1-8B-Instruct\" \\\n  --quantized_model \"user_data/jobs/QServe/qoq_quantization\" \\\n  --tasks \"gptq_wikitext:0\" \\\n  --results \"user_data/evals/meta-llama_3.1-8b\" \\\n  --cache_dir \"user_data/.cache\"\n\n# baseline arc_challenge 25 shot evaluation\naccelerate launch --no-python lm_eval --model hf \\\n  --model_args pretrained=meta-llama/Meta-Llama-3.1-8B-Instruct,cache_dir=user_data/.cache \\\n  --tasks arc_challenge \\\n  --num_fewshot 25 \\\n  --batch_size 16 \\\n  --output_path user_data/evals/meta-llama_3.1-8b/base/arc_challenge/\n\n# quantized arc_challenge 25 shot evaluation\naccelerate launch --no-python lm_eval --model hf \\\n  --model_args pretrained=user_data/jobs/QServe/qoq_quantization,cache_dir=user_data/.cache \\\n  --tasks arc_challenge \\\n  --num_fewshot 25 \\\n  --batch_size 16 \\\n  --output_path user_data/evals/meta-llama_3.1-8b/quantized/arc_challenge/\n</code></pre> <p>Compare the results with the original model to assess the impact of quantization on accuracy and inference speed.</p>"},{"location":"examples/text-generation/lmquant_quantization/readme/#performance-metrics","title":"Performance Metrics","text":"Model Task Metric Baseline Quantized Impact Llama3.1-8b Perplexity (wikitext, gptq) Perplexity \u2193 6.14 6.76 +0.62 Llama3.1-8b ARC Challenge (25 shot) Accuracy \u2191 60.66 60.10 -0.56 Llama3.1-8b Size (in GB) Model Size \u2193 15.0 05.1 -09.90"},{"location":"examples/text-generation/lmquant_quantization/readme/#throughput-comparison","title":"Throughput Comparison","text":"A100 (80G) TRT-LLM-FP16 TRT-LLM-W4A16 TRT-LLM-W8A8 QServe-W4A8KV4 Throughput Increase* Llama-3-8B 2503 2370 2396 3005 1.20x <p>*</p>"},{"location":"examples/text-generation/lmquant_quantization/readme/#conclusion","title":"Conclusion","text":"<p>LMQuant\u2019s QoQ algorithm offers a robust method for compressing large models like Llama3.1-8b, balancing performance with efficiency. By following this guide, you should now have a quantized model ready for deployment in resource-constrained environments.</p> <p>Author: Kushwaha, Shubham</p>"},{"location":"examples/text-generation/lmquant_quantization/readme/#additional-examples","title":"Additional Examples","text":"<ul> <li>Maximising math performance for extreme compressions: 2-bit Llama3-8b (w2a16)</li> <li>Efficient 4-bit Quantization (w4a16) of Llama3.1-8b for Optimized Text Generation</li> <li>Llama3.1 70B: 0.5x the cost &amp; size</li> <li>Accelerating a 4-bit Quantised Llama Model</li> </ul>"},{"location":"examples/text-generation/tensorrtllm_engine/readme/","title":"Accelerating a 4-bit Quantised Llama Model","text":""},{"location":"examples/text-generation/tensorrtllm_engine/readme/#overview","title":"Overview","text":"<p>This guide demonstrates how to accelerate a 4-bit quantized Llama model using the TensorRTLLM engine. TensorRTLLM is a high-performance inference engine that leverages NVIDIA's TensorRT library to optimize and accelerate models for deployment on NVIDIA GPUs.</p>"},{"location":"examples/text-generation/tensorrtllm_engine/readme/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Requirements</li> <li>Installation</li> <li>Configuration</li> <li>Running the engine build</li> <li>Performance Evaluation</li> <li>Conclusion</li> </ul>"},{"location":"examples/text-generation/tensorrtllm_engine/readme/#introduction","title":"Introduction","text":"<p>In this example, we'll demonstrate how to accelerate a 4-bit quantized model using the TensorRTLLM engine. The process involves quantizing the model using the AWQ quantization technique and then optimizing it for deployment on NVIDIA GPUs using TensorRTLLM.</p>"},{"location":"examples/text-generation/tensorrtllm_engine/readme/#requirements","title":"Requirements","text":"<p>Before you begin, ensure that you have the following:</p> <ul> <li>A GPU-enabled environment with CUDA support.</li> </ul>"},{"location":"examples/text-generation/tensorrtllm_engine/readme/#installation","title":"Installation","text":""},{"location":"examples/text-generation/tensorrtllm_engine/readme/#step-1-clone-the-nyuntam-repository","title":"Step 1: Clone the Nyuntam Repository","text":"<p>Clone the repository and navigate to the <code>nyuntam</code> directory:</p> <pre><code>git clone https://github.com/nyunAI/nyuntam.git\ncd nyuntam/examples/text-generation/tensorrtllm_engine/\n</code></pre>"},{"location":"examples/text-generation/tensorrtllm_engine/readme/#step-2-set-up-the-workspace","title":"Step 2: Set Up the workspace","text":"<p>Create and activate an environment for the AWQ quantization example:</p> <pre><code>conda create -n tensorrtllm_engine python=3.10 -y # or use virtualenv if preferred\nconda activate tensorrtllm_engine\n</code></pre> <p>Install the required dependencies:</p> <pre><code>pip install git+https://github.com/nyunAI/nyunzero-cli.git\n</code></pre> <p>Setup the nyun workspace</p> <pre><code>mkdir workspace &amp;&amp; cd workspace\nnyun init -e kompress-text-generation # wait for the extensions to be installed\n</code></pre>"},{"location":"examples/text-generation/tensorrtllm_engine/readme/#configuration","title":"Configuration","text":"<p>Prepare the YAML configuration file specific to AWQ quantization. Use the following template as a starting point:</p> <pre><code># tensorrtllm_engine.yaml\n\n# Model configuration\nMODEL: \"meta-llama/Llama-2-7b-hf\"\n\n# Data configuration\nDATASET_NAME: \"wikitext\"\nDATASET_SUBNAME: \"wikitext-2-raw-v1\"\nTEXT_COLUMN: \"text\"                     \nSPLIT: \"train\"\n\nDATA_PATH:\nFORMAT_STRING:\n\n# Acceleration configuration\nllm:\n  TensorRTLLM:\n    to_quantize: true # to first quantize the model and then build engine. (Supported only for llama, gptj, &amp; falcon models.)\n    dtype: float16\n\n    # quantization parameters\n    quant_method: \"int4_awq\" # 'fp8', 'int4_awq', 'smoothquant', 'int8'\n    smoothquant: 0.5 # in case smoothquant value is given\n    calib_size: 32\n\n    ...other params\n\n# Job configuration\nCUDA_ID: \"0\"\nALGORITHM: \"TensorRTLLM\"\nJOB_SERVICE: \"Kompress\"\nUSER_FOLDER: \"/user_data/example\"\nJOB_ID: \"tensorrtllm_engine\"\nCACHE_PATH: \"/user_data/example/.cache\"\nJOB_PATH: \"/user_data/example/jobs/tensorrtllm_engine\"\nLOGGING_PATH: \"/user_data/example/logs/tensorrtllm_engine\"\nALGO_TYPE: \"llm\"\nTASK: \"llm\"\n</code></pre>"},{"location":"examples/text-generation/tensorrtllm_engine/readme/#running-the-engine-build","title":"Running the engine build","text":"<p>With your YAML file configured, initiate the process by running:</p> <pre><code>nyun run ../config.yaml\n</code></pre> <p>Monitor the process to ensure that the quantization completes successfully.</p> <p>Once the job starts, you'll find the following directory structure in the <code>workspace</code> folder:</p> <pre><code>workspace/\n\u251c\u2500\u2500 custom_data\n\u2514\u2500\u2500 example\n    \u251c\u2500\u2500 datasets\n    \u2502   \u2514\u2500\u2500 wikitext\n    \u251c\u2500\u2500 jobs\n    \u2502   \u2514\u2500\u2500 Kompress\n    \u2502       \u2514\u2500\u2500 tensorrtllm_engine\n    \u251c\u2500\u2500 logs\n    \u2502   \u2514\u2500\u2500 tensorrtllm_engine\n    \u2514\u2500\u2500 models\n        \u2514\u2500\u2500 meta-llama\n            \u2514\u2500\u2500 Llama-2-7b-hf\n                ...\n</code></pre> <p>The output model will be saved in the <code>workspace/example/jobs/Kompress/tensorrtllm_engine</code> directory.</p>"},{"location":"examples/text-generation/tensorrtllm_engine/readme/#performance-evaluation","title":"Performance Evaluation","text":"<p>Following is the comparison of the results* with the original model to assess the impact of quantization on accuracy and inference speed.</p> Model Optimised with Quantization Type WM (GB) RM (GB) Tokens/s Perplexity meta-llama/Llama-2-7b-hf TensorRT-LLM AWQ GEMM 4bit (quant_method=int4_awq) 3.42 5.69 194.86 6.02 INT8 (quant_method=int8) 6.53 8.55 143.57 5.89 FP16 (to_quantize=false) 12.55 14.61 83.43 5.85 meta-llama/Llama-2-7b-hf Text-Generation-Inference AWQ GEMM 4bit 3.62 36.67 106.84 6.02 FP16 12.55 38.03 74.19 5.85 <p>*Source: Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward</p>"},{"location":"examples/text-generation/tensorrtllm_engine/readme/#conclusion","title":"Conclusion","text":"<p>In this example, we demonstrated how to accelerate a 4-bit quantized Llama3.1-8b model using the TensorRTLLM engine. By leveraging the nyun cli, we optimized the model for deployment on NVIDIA GPUs, achieving significant improvements in inference speed and memory efficiency.</p> <p>Author: Kushwaha, Shubham</p>"},{"location":"examples/text-generation/tensorrtllm_engine/readme/#additional-examples","title":"Additional Examples","text":"<ul> <li>Maximising math performance for extreme compressions: 2-bit Llama3-8b (w2a16)</li> <li>Efficient 4-bit Quantization (w4a16) of Llama3.1-8b for Optimized Text Generation</li> <li>Llama3.1 70B: 0.5x the cost &amp; size</li> <li>Achieving Up to 2.5x TensorRTLLM Speedups: Efficient 4-8-4 Quantization (w4a8kv4) of Llama3.1-8b</li> </ul>"},{"location":"examples/vision/mmrazor_pruning/readme/","title":"Readme","text":""},{"location":"examples/vision/mmrazor_pruning/readme/#mmrazor-pruning-on-yolox-using-nyuncli","title":"MMRazor Pruning on YOLOX using NyunCLI","text":"<p>Pruning Object Detection is supported via MMRazor Pruning. Currently activation based pruning and flop based pruning are the available option. This example utilizes NyunCLI a 1-click solution to run the pruning job. We need to build a configuration for the job specifying parameters. The steps for pruning a YOLOX is as follows. Starting YAML Here</p>"},{"location":"examples/vision/mmrazor_pruning/readme/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Downloading NyunCLI </li> <li>Downloading the Dataset </li> <li>Model Loading </li> <li>Loading Pretrained Weights (Optional)</li> <li>Modifying Hyperparameters </li> <li>Starting the Job </li> <li>Results</li> </ol>"},{"location":"examples/vision/mmrazor_pruning/readme/#1-downloading-nyuncli","title":"1. Downloading NyunCLI","text":"<p>Nyun-cli offers the users the luxury to compress their neural networks via a single command line. To download and install NyunCLI.</p> <pre><code>pip install git+https://github.com/nyunAI/nyunzero-cli.git\nnyun init {WORKSPACE_PATH} \"\"\n</code></pre> <p>Here <code>WORKSPACE_PATH</code> is the root folder for running the experimentation and <code>CUSTOM_DATA_PATH</code> defines the path of custom data. The <code>YAML_PATH</code> is the path of configuration to be used. The base configuration used for this example can be found at vision scripts.This example offers a basic example for  installing nyuncli visit nyuncli documentation for a more advanced installation.</p>"},{"location":"examples/vision/mmrazor_pruning/readme/#2-downloading-the-dataset","title":"2. Downloading the dataset","text":"<p>Downloading and formatting CIFAR-10 dataset is automated via Nyuntam. We need to specify the following hyperparameters in the YAML configuration </p> <pre><code>DATASET_NAME: COCODETECTION\nDATASET_PATH: {DATASET_PATH}\nTASK: object_detection\n</code></pre> <p>Note: Dataset Path is expected to to be the path relative to WORKSPACE_PATH</p>"},{"location":"examples/vision/mmrazor_pruning/readme/#3-model-loading","title":"3. Model Loading","text":"<p>Nyuntam supports object detection models from MMDet and MMYolo libraries, we load YOLOX-tiny from MMDet for this experiment. Pretrained COCO weights and model config are internally downloaded during job run. NOTE: the model name argument must match the exact name used in mmdet configs. </p> <pre><code>MODEL_NAME: yolox_tiny_8xb8-300e_coco\nPLATFORM: mmdet\n</code></pre>"},{"location":"examples/vision/mmrazor_pruning/readme/#4-loading-pretrained-weights-optional","title":"4. Loading Pretrained Weights (Optional)","text":"<p>You can optionally load pretrained weights if you already have the same.  This can be used to prune mmdet and mmyolo models trained on custom datasets. </p> <pre><code>CUSTOM_MODEL_PATH: {CUSTOM_MODEL_PATH}\n#leave CUSTOM_MODEL_PATH as \"\" if unused\n</code></pre>"},{"location":"examples/vision/mmrazor_pruning/readme/#5-setting-task-specific-hyperparameters","title":"5. Setting Task Specific Hyperparameters","text":"<p>We specify the hyperparameters used for running this tutorial below, we may modify them to better suit your pruning task, more details on the hyperparameters can be found at Nyun Documentation.</p> <pre><code>    INTERVAL: 10 # The interval between two pruning operations\n    NORM_TYPE: 'act' # Type of importance , activation or flops\n    LR_RATIO: 0.1 # Learning Rate Ratio\n    TARGET_FLOP_RATIO: 0.9 # Target Number of Flops value * number of existing flops.\n    EPOCHS: 5 # Total Number of Epochs to perform pruning (stops early once reached required flops)\n</code></pre>"},{"location":"examples/vision/mmrazor_pruning/readme/#6-starting-the-job","title":"6. Starting the Job","text":"<p>The following command starts the job using nyun-cli. </p> <pre><code>nyun run {CONFIG_PATH}\n</code></pre> <p><code>CONFIG Path</code> Config path is the edited yaml used for defining the hyperparameters of the job. </p> <p>After the job is completed the folder structure at Workspace_path will be as follows</p> <pre><code>{WORKSPACE}\n\u251c\u2500\u2500 datasets\n\u2502   \u251c\u2500\u2500 {JOB_ID}\n|   |   \u251c\u2500\u2500root\n\u2502   |   |   \u251c\u2500\u2500 train\n\u2502   |   |   \u251c\u2500\u2500 val\n\u2502   |   |   \u251c\u2500\u2500 annotations\n\u251c\u2500\u2500 logs\n\u2502   \u251c\u2500\u2500 log.log\n\u251c\u2500\u2500 jobs\n\u2502   \u251c\u2500\u2500 mds.pt\n\u2502   \u251c\u2500\u2500 flops_{target_flop_ratio}.pth\n|   \u251c\u2500\u2500 fix_subnet.json\n</code></pre>"},{"location":"examples/vision/mmrazor_pruning/readme/#7-results","title":"7. Results","text":"<p>We use MAP and Latency to assess the result produced.  To calculate the latency and map post pruning you can use mmrazor's test.py using the following code</p> <pre><code>cd {mmrazor tools folder}\npython test.py {cache_path}/current_fisher_finetune_config.py.py {path to mds.pt} {batch_size}\n</code></pre> <p>The results are as follows:</p> Model MAP Latency Unpruned 32.0 0.35 Prune+Finetune 29.6 0.31 ms"},{"location":"examples/vision/nncf_quantization/readme/","title":"Readme","text":""},{"location":"examples/vision/nncf_quantization/readme/#8-bit-cpu-quantization-of-resnet50-using-nncf-on-cifar-10-dataset-via-nyuncli-installation","title":"8-bit CPU Quantization of ResNet50 using NNCF on CIFAR-10 Dataset via NyunCLI Installation","text":"<p>8-bit quantization of Image Classification models are done via NNCF, ONNX Quantizers for CPU and TensorRT Quantizer for GPU deployment. This example shows how to Quantize a ResNet50 model with NNCF for CPU Quantization via nyuncli </p>"},{"location":"examples/vision/nncf_quantization/readme/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Downloading NyunCLI </li> <li>Downloading the Dataset </li> <li>Model Loading </li> <li>Loading Pretrained Weights (Optional)</li> <li>Finetuning Model Before Training (Optional) </li> <li>Starting the Job </li> <li>Results</li> </ol>"},{"location":"examples/vision/nncf_quantization/readme/#1-downloading-nyuncli","title":"1. Downloading NyunCLI","text":"<p>Nyun-cli offers the users the luxury to compress their neural networks via a single command line. To download and install NyunCLI.</p> <pre><code>pip install git+https://github.com/nyunAI/nyunzero-cli.git\nnyun init {WORKSPACE_PATH} \"\"\n</code></pre> <p>Here <code>WORKSPACE_PATH</code> is the root folder for running the experimentation and <code>CUSTOM_DATA_PATH</code> defines the path of custom data. The <code>YAML_PATH</code> is the path of configuration to be used. The base configuration used for this example can be found at vision scripts.This example offers a basic example for  installing nyuncli visit nyuncli documentation for a more advanced installation.</p>"},{"location":"examples/vision/nncf_quantization/readme/#2-downloading-the-dataset","title":"2. Downloading the dataset","text":"<p>Downloading and formatting CIFAR-10 dataset is automated via Nyuntam. We need to specify the following hyperparameters in the YAML configuration </p> <pre><code>DATASET_NAME: CIFAR10\nDATASET_PATH: {DATASET_PATH}\nTASK: image_classification\n</code></pre> <p>Note: Dataset Path is expected to to be the path relative to WORKSPACE_PATH</p>"},{"location":"examples/vision/nncf_quantization/readme/#3-model-loading","title":"3. Model Loading","text":"<p>Nyuntam supports most Classification models supported via HuggingFace, Timm and Torchvision libraries. We load ResNet50 from torchvision in this experiment. The following parameters in the YAML configuration are to be updated</p> <pre><code>MODEL_NAME: resnet50\nPLATFORM: torchvision\n</code></pre>"},{"location":"examples/vision/nncf_quantization/readme/#4-loading-pretrained-weights-optional","title":"4. Loading Pretrained Weights (Optional)","text":"<p>You can optionally load pretrained weights if you already have the same. In case of this tutorial we would be finetuning the CIFAR 10 on 10 epochs instead (see sec.5)</p> <pre><code>CUSTOM_MODEL_PATH: {CUSTOM_MODEL_PATH}\n#leave CUSTOM_MODEL_PATH as \"\" if unused\n</code></pre>"},{"location":"examples/vision/nncf_quantization/readme/#5-finetuning-model-before-training-optional","title":"5. Finetuning Model Before Training (Optional)","text":"<p>You can optionally fine-tune the model with these parameters to tune in with the custom dataset. </p> <pre><code>TRAINING: True\nLEARNING_RATE: 0.001\nFINETUNE_EPOCHS: 10\nVALIDATE: True\nVALIDATION_INTERVAL: 1\n</code></pre>"},{"location":"examples/vision/nncf_quantization/readme/#6-starting-the-job","title":"6. Starting the Job","text":"<p>The following command starts the job using nyun-cli. The NNCF </p> <pre><code>nyun run {CONFIG_PATH}\n</code></pre> <p><code>CONFIG Path</code> Config path is the edited yaml used for defining the hyperparameters of the job. </p> <p>After the job is completed the folder structure will be as follows</p> <pre><code>{WORKSPACE}\n\u251c\u2500\u2500 datasets\n\u2502   \u251c\u2500\u2500 {JOB_ID}\n\u2502   |   \u251c\u2500\u2500 train\n\u2502   |   \u251c\u2500\u2500 val\n\u251c\u2500\u2500 logs\n\u2502   \u251c\u2500\u2500 log.log\n\u251c\u2500\u2500 jobs\n\u2502   \u251c\u2500\u2500 mds.xml\n\u2502   \u251c\u2500\u2500 mds.bin\n</code></pre>"},{"location":"examples/vision/nncf_quantization/readme/#7-results","title":"7. Results","text":"<p>The model post finetuning on cifar-10 for 10 epochs had validation 67.4 % validation accuracy and after quantization had an accuracy of 67.2 %. To benchmark the reduced latency of the model use NNCF Benchmarking tool using the following command. </p> <pre><code>benchmark_app -m {path to .xml file} -d CPU -api async\n</code></pre> <p>The Final Results are as follows:</p> Model Accuracy Latency FP32 67.7 1.32 ms INT8 67.2 0.68 ms"},{"location":"nyuntam_adapt/","title":"Adapt","text":""},{"location":"nyuntam_adapt/#overview","title":"Overview","text":"<p>Adapt is a no-code toolkit that enables Parameter-Efficient Fine-Tuning (PEFT) techniques such as LoRA, DoRA, QLoRA, and SSF for AI model fine-tuning across various tasks, including language and vision. Designed as a simple no-code platform, it simplifies the application of PEFT to enhance model performance for tasks like text classification, generation, summarization, question answering, image classification, object detection, and segmentation. Adapt democratizes advanced AI customization, making it accessible to a broader audience.</p>"},{"location":"nyuntam_adapt/#basic-workflow","title":"Basic Workflow","text":""},{"location":"nyuntam_adapt/#step-1-import-a-model-and-dataset","title":"Step 1 - Import a model and dataset.","text":"<p>Check Import Data and for the exact steps. Make sure that the model-dataset combination provided is valid and of the same task.</p>"},{"location":"nyuntam_adapt/#step-2-choose-an-algorithm","title":"Step 2 - Choose an Algorithm.","text":"<p>Check available Algorithms and respective hyperparameters. By default, most optimal hyperparameters are chosen, however, depending upon the task, model and dataset another set of hyperparameters can work better too.</p>"},{"location":"nyuntam_adapt/#step-3-monitor-logs-and-export","title":"Step 3 - Monitor Logs and Export.","text":"<p>By default, Job logs and model checkpoints are saved in Adapt/user_data/, however, users can view the logs and download their Adapted models locally using the export functionality.</p>"},{"location":"nyuntam_adapt/algorithms/","title":"Algorithms","text":""},{"location":"nyuntam_adapt/algorithms/#overview","title":"Overview","text":"<p>nyuntam_adapt currently supports the following tasks - </p> <ul> <li>Text Generation</li> <li>Text Classification</li> <li>Summarization</li> <li>Translation</li> <li>Question Answering</li> <li>Image Classification</li> <li>Object Detection</li> <li>Instance Segmentation</li> <li>Pose Detection</li> </ul> <p>The following techniques are supported to adapt any model for the above mentioned tasks - </p> <ul> <li>LoRA</li> <li>SSF</li> <li>DoRA</li> <li>QLoRA (4-bit and 8-bit)</li> <li>QDoRA (4-bit and 8-bit)</li> <li>QSSF (4-bit)</li> </ul>"},{"location":"nyuntam_adapt/algorithms/#lora-low-rank-adaptation","title":"LoRA (Low Rank Adaptation)","text":"<p>LoRA is a Parameter-Efficient Fine-Tuning (PEFT) technique that optimizes AI models by introducing low-rank matrices to adjust the weights of pre-trained neural networks. This approach allows for significant improvements in model performance with minimal additional parameters, making it an efficient method for customizing AI models for specific tasks without the need for extensive retraining or computational resources.</p> <p>Argument : method = \"LoRA\"</p> Parameter Value (Datatype) Default Value Description <code>r</code> int 8 Rank of the low-rank approximation. <code>alpha</code> int 16 Scaling factor for LoRA adjustments. <code>dropout</code> float 0.1 Dropout rate for regularization. <code>target_modules</code> Modules within the model targeted for tuning. <code>fan_in_fan_out</code> bool False Whether to adjust initialization based on fan-in/fan-out. <code>init_lora_weights</code> bool True Initializes LoRA weights if set to True."},{"location":"nyuntam_adapt/algorithms/#ssf-scaling-and-shifting-your-features","title":"SSF (Scaling and Shifting your Features)","text":"<p>SSF adjusts the scale (multiplication) and shift (addition) of features within a neural network to better adapt the model to specific tasks or datasets. By applying these simple yet effective transformations, SSF aims to enhance model performance without the need for extensive retraining or adding a significant number of parameters. This technique is particularly useful for fine-tuning pre-trained models in a more resource-efficient manner, allowing for targeted improvements with minimal computational cost.</p> <p>Argument : method = \"SSF\"</p>"},{"location":"nyuntam_adapt/algorithms/#dora-weight-decomposed-low-rank-adaptation","title":"DoRA (Weight-Decomposed Low-Rank Adaptation)","text":"<p>DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. DoRA enhances both the learning capacity and training stability of LoRA while avoiding any additional inference overhead.</p> Parameter Datatype Default Value Description <code>r</code> integer 16 Rank of the low-rank approximation. <code>alpha</code> integer 8 Scaling factor for LoRA adjustments. <code>dropout</code> float 0.1 Dropout rate for regularization. <code>target_modules</code> list The names of the layers for which peft modules will be created <code>fan_in_fan_out</code> boolean False Initializes DoRA weights if set to True. <p>Argument : method = \"DoRA\"</p>"},{"location":"nyuntam_adapt/algorithms/#qlora-quantized-low-rank-adaptation","title":"QLoRA (Quantized Low Rank Adaptation)","text":"<p>QLoRA (Quantized Low-Rank Adaptation) is a sophisticated Parameter-Efficient Fine-Tuning (PEFT) technique designed to enhance the adaptability and efficiency of pre-trained AI models with minimal computational overhead. The weight matrices of the original model are frozen and quantized thus reducing the memory footprint of the original model. QLoRA is particularly useful in scenarios where computational resources are limited, offering a balance between model adaptability, performance, and resource efficiency.</p> <p>Arguments : <ul> <li>method = \"LoRA\"</li> <li>load_in_4bit = \"True\" (for 4 bit quantization)</li> <li>load_in_8bit = \"True\" (for 8 bit quantization)</li></ul></p>"},{"location":"nyuntam_adapt/algorithms/#qdora-weight-decomposed-low-rank-adaptation","title":"QDoRA (Weight-Decomposed Low-Rank Adaptation)","text":"<p>QDoRA (Weight-Decomposed Low-Rank Adaptation) is a sophisticated Parameter-Efficient Fine-Tuning (PEFT) technique designed to enhance the adaptability and efficiency of pre-trained AI models with minimal computational overhead.  QDoRA is particularly useful in scenarios where computational resources are limited, offering a balance between model adaptability, performance, and resource efficiency.</p> <p>Arguments : <ul> <li>method = \"DoRA\"</li> <li>load_in_4bit = \"True\" (for 4 bit quantization)</li> <li>load_in_8bit = \"True\" (for 8 bit quantization)</li></ul></p>"},{"location":"nyuntam_adapt/algorithms/#q-ssf-quantized-ssf","title":"Q-SSF (Quantized SSF)","text":"<p>Quantized SSF applies quantizes the frozen model weights. The SSF modules are trained, enhancing model efficiency with minimal fidelity loss. This approach reduces memory and computational demands, ideal for resource-constrained environments, maintaining accuracy while improving performance.</p> <p>Arguments : <ul> <li>method = \"SSF\"</li> <li>load_in_4bit = \"True\" (for 4 bit quantization)</li> <li>load_in_8bit = \"True\" (for 8 bit quantization)</li></ul></p>"},{"location":"nyuntam_adapt/algorithms/#quantization-parameters-for-qloraqdoraq-ssf","title":"Quantization Parameters for QLoRA/QDoRA/Q-SSF","text":"<p>Model Quantization is achieved via BitsandBytes module.</p> <p>4 bit Quantization</p> Parameter Datatype Default Value Description <code>load_in_4bit</code> bool True Load model in 4-bit precision. <code>bnb_4bit_compute_dtype</code> str 'float16' Compute data type in 4-bit mode. <code>bnb_4bit_quant_type</code> str 'nf4' Quantization type for 4-bit precision. <code>bnb_4bit_use_double_quant</code> bool False Use double quantization in 4-bit mode. <p>8 bit Quantization</p> Parameter Datatype Default Value Description <code>load_in_8bit</code> bool False Load model in 8-bit precision (only for float16/bfloat16 weights). <code>llm_int8_threshold</code> float 6.0 Threshold for LLM int8 quantization. <code>llm_int8_skip_modules</code> Modules to skip during LLM int8 quantization. <code>llm_int8_enable_fp32_cpu_offload</code> bool False Enable FP32 offload to CPU in int8 mode. <code>llm_int8_has_fp16_weight</code> bool False Whether the model has fp16/bfloat16 weights"},{"location":"nyuntam_adapt/algorithms/#full-fine-tuning","title":"Full Fine-tuning","text":"<p>This method updates all parameters of a neural network. This method is generally inefficient to run as it uses a lot of computing power to update all parameters of the given model. </p> <p>Arguments : <ul> <li>FULL_FINE_TUNING = \"True\"</li> </ul></p>"},{"location":"nyuntam_adapt/algorithms/#last-layer-tuning","title":"Last Layer Tuning","text":"<p>Tuning the last layer is a popular technique in which all the layers of a model, except the last few layers are frozen i.e. their gradients are not updated during training. This method is generally used while finetuning a model trained on a huge generalized dataset (Imagenet) for downstream tasks.  Arguments : <ul> <li>LAST_LAYER_TUNING = \"True\"</li> </ul></p> <p>*NOTE :<ul><li>Full Fine Tuning should be used without any PEFT methods.</li><li>Last Layer Tuning should be set to \"True\" while using any PEFT method.</li> </ul></p>"},{"location":"nyuntam_adapt/algorithms/#common-training-parameters","title":"Common Training Parameters","text":"<p>The wide variety of tasks and algorithms supported in nyuntam_adapt have several parameters that are common across all tasks/algorithms and some parameters that are specific to a particular task/algorithm</p> Parameter Datatype Default Value Description <code>DO_TRAIN</code> bool true Flag indicating whether to perform training <code>DO_EVAL</code> bool true Flag indicating whether to perform evaluation <code>NUM_WORKERS</code> int 4 Number of worker processes for data loading <code>BATCH_SIZE</code> int 16 Batch size for training <code>EPOCHS</code> int 2 Number of epochs for training <code>OPTIMIZER</code> str adamw_torch Optimization algorithm (sgd,adamw_torch,paged_adamw_32bit) <code>LR</code> float 1e-4 Learning rate <code>SCHEDULER_TYPE</code> str linear Type of learning rate scheduler(linear,constant) Supported Schedulers : <ul><li>Object deteciton, Instance Segmentation, Pose Detection - <ul><li>CosineAnnealingLR</li> <li>LinearLR</li> <li>MultiStepLR</li> </ul><li> NLP Tasks, Image Classification - All huggingface schedulers <code>WEIGHT_DECAY</code> float 0.0 Weight decay for optimization <code>BETA1</code> float 0.9 Beta1 parameter for Adam optimizer <code>BETA2</code> float 0.999 Beta2 parameter for Adam optimizer <code>ADAM_EPS</code> float 1e-8 Epsilon value for Adam optimizer <code>INTERVAL</code> str epoch Interval type for checkpointing (e.g., epoch) <code>INTERVAL_STEPS</code> int 100 Steps interval for checkpointing <code>NO_OF_CHECKPOINTS</code> int 5 Number of checkpoints to save during training <code>FP16</code> bool false Flag indicating whether to use FP16 precision <code>RESUME_FROM_CHECKPOINT</code> bool false Flag indicating whether to resume from a checkpoint <code>GRADIENT_ACCUMULATION_STEPS</code> int 1 Number of steps to accumulate gradients <code>GRADIENT_CHECKPOINTING</code> bool false Flag indicating whether to use gradient checkpointing <code>SAVE_METHOD</code> string 'state_dict' The method in which the model will be saved (Values - 'full_torch_model' : Saves the model as a .pt file in full precision, 'state_dict' : Saves the model state dictionary,  'safetensors' : Saves the model weights as safetensors (Advisable for huggingface models) ,'save_pretrained':saves the model as a folder using huggingface's save_pretrained method (Only supporte for huggingface models.)  )"},{"location":"nyuntam_adapt/algorithms/#ddp-arguments","title":"DDP arguments","text":"<p>DistributedDataParallel (DDP) implements data parallelism at the module level which can run across multiple machines. The recommended way to run adaption for a model with DDP is to use the following parameters :</p> Parameter Data Type Default Value Description <code>DDP</code> bool True A boolean indicating whether DDP is enabled or not. <code>num_nodes</code> int 1 An integer refering to the number of nodes used for DDP training"},{"location":"nyuntam_adapt/algorithms/#fsdp-arguments","title":"FSDP arguments","text":"<p>Training AI models at a large scale is a challenging task that requires a lot of compute power and resources. It also comes with considerable engineering complexity to handle the training of these very large models. To address this Fully Sharded Data Parallel (FSDP) comes inbuilt with nyuntam-adapt. When training with FSDP, the GPU memory footprint is smaller than when training with DDP across all workers. This makes the training of some very large models feasible by allowing larger models or batch sizes to fit on device. This comes with the cost of increased communication volume. The communication overhead is reduced by internal optimizations like overlapping communication and computation.</p> Parameter Name Data Type Default Value Description <code>FSDP</code> Bool True Boolean indicating whether FSDP is enabled or not. <code>compute_environment</code> String LOCAL_MACHINE Specifies the environment where the computation runs. <code>debug</code> Boolean false Indicates whether debugging is enabled. <code>distributed_type</code> String FSDP Defines the type of distributed training. <code>downcast_bf16</code> String 'no' Controls downcasting to bf16 precision. <code>fsdp_auto_wrap_policy</code> String TRANSFORMER_BASED_WRAP Auto-wrap policy for FSDP. <code>fsdp_backward_prefetch_policy</code> String BACKWARD_PRE Prefetch policy during FSDP backward pass. <code>fsdp_forward_prefetch</code> Boolean false Indicates if forward prefetching is enabled. <code>fsdp_cpu_ram_efficient_loading</code> Boolean true Enables efficient loading into CPU RAM. <code>fsdp_offload_params</code> Boolean false Indicates if parameters are offloaded. <code>fsdp_sharding_strategy</code> String FULL_SHARD Defines the FSDP sharding strategy. <code>fsdp_state_dict_type</code> String SHARDED_STATE_DICT Type of state dictionary used in FSDP. <code>fsdp_sync_module_states</code> Boolean true Synchronizes module states across processes. <code>fsdp_use_orig_params</code> Boolean true Indicates if original parameters are used. <code>machine_rank</code> Integer 0 Rank of the machine in the distributed setup. <code>main_training_function</code> String main Name of the main training function to run. <code>mixed_precision</code> String bf16 Specifies the mixed precision type. <code>num_machines</code> Integer 1 Number of machines used in training. <code>num_processes</code> Integer 2 Number of processes per machine. <code>rdzv_backend</code> String static Rendezvous backend used for distributed training. <code>same_network</code> Boolean true Indicates if machines are on the same network. <code>tpu_env</code> Array [] Environment variables for TPU. <code>tpu_use_cluster</code> Boolean false Indicates if TPU cluster usage is enabled. <code>tpu_use_sudo</code> Boolean false Indicates if sudo is required for TPU usage. <code>use_cpu</code> Boolean false Indicates if CPU is used instead of GPU/TPU."},{"location":"nyuntam_adapt/algorithms/#text-generation","title":"Text Generation","text":"Parameter Data Type Default Value Description <code>packing</code> bool True A boolean indicating whether packing is enabled or not. <code>dataset_text_field</code> str 'text' The field in the dataset containing the text data. <code>max_seq_length</code> int 512 The maximum sequence length allowed for input text. <code>flash_attention2</code> bool Fasle Argument to indicate whether to use flash attention or not (Warning - most of the models don't support flash attention which might lead to unexpected behaviours)"},{"location":"nyuntam_adapt/algorithms/#text-classification","title":"Text Classification","text":"<p>nyuntam_adapt supports <ul> <li> Token Classification : <ul> <li> Named entity recognition (NER) : Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for \u201cno entity.\u201d </li> <li> Part-of-speech tagging (POS): Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.). </li> <li> Chunking: Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually B-) to any tokens that are at the beginning of a chunk, another label (usually I-) to tokens that are inside a chunk, and a third label (usually O) to tokens that don\u2019t belong to any chunk.</li> </ul> <li> Text Classification : Classification of given text into 2 or more classes (examples - emotion recognition)</li> Parameter Data Type Default Value Description <code>subtask</code> str None The specific subtask associated with the model (\"ner\" ,\"pos\", \"chunk\", ). If subtask = None, then the task is classic text classification"},{"location":"nyuntam_adapt/algorithms/#summarization","title":"Summarization","text":"Parameter Data Type Default Value Description <code>MAX_TRAIN_SAMPLES</code> int 1000 Maximum number of training samples <code>MAX_EVAL_SAMPLES</code> int 1000 Maximum number of evaluation samples <code>max_input_length</code> int 512 The maximum length allowed for input documents. <code>max_target_length</code> int 128 The maximum length allowed for generated summaries. <code>eval_metric</code> str 'rouge' The evaluation metric used during training and evaluation (options: 'bleu', 'rouge'). <code>generation_max_length</code> int 128 The maximum length allowed for generated text during prediction."},{"location":"nyuntam_adapt/algorithms/#translation","title":"Translation","text":"Parameter Data Type Default Value Description <code>max_input_length</code> int 128 The maximum length allowed for input sentences. <code>max_target_length</code> int 128 The maximum length allowed for translated sentences. <code>eval_metric</code> str 'rouge' The evaluation metric used during training and evaluation (options: 'sacrebleu', 'rouge'). <code>source_lang</code> str 'en' The source language for translation (e.g., English). <code>target_lang</code> str 'ro' The target language for translation (e.g., Romanian). <code>PREFIX</code> str example -'translate English to Russian: ' For multi-task models like t5, prefix is attached during specific tasks"},{"location":"nyuntam_adapt/algorithms/#question-answering","title":"Question Answering","text":"<p>nyuntam_adapt support span detection in question answering.</p> Parameter Data Type Default Value Description <code>MAX_TRAIN_SAMPLES</code> int 1000 Maximum number of training samples <code>MAX_EVAL_SAMPLES</code> int 1000 Maximum number of evaluation samples <code>max_answer_length</code> int 30 The maximum length allowed for the generated answers. <code>max_length</code> int 384 The maximum length allowed for input documents. <code>doc_stride</code> int 128 The stride used when the context is too large and is split across several features"},{"location":"nyuntam_adapt/algorithms/#image-classification","title":"Image Classification","text":"Parameter Data Type Default Value Description <code>load_model</code> bool False A boolean indicating whether to load a pre-trained model. <code>model_path</code> str \"densenet121\" The path or identifier of the pre-trained model to be loaded. <code>model_type</code> str 'densenet_timm' The type of the loaded model (e.g., 'densenet_timm'). <code>image_processor_path</code> str 'facebook/convnext-tiny-224' The path or identifier of the image processor configuration."},{"location":"nyuntam_adapt/algorithms/#object-detection","title":"Object Detection","text":"Parameter Datatype Default Value Description <code>BEGIN</code> int 0 The epoch from which the learning rate scheduler starts <code>END</code> int 50 The epoch at which the learning rate scheduler stops <code>T_MAX</code> int 0 Maximum number of iterations. (Exclusive Parameter for CossineAnnealingLR scheduler) <code>WARMUP</code> bool false Flag to indicate whether to use wamrup iters or not <code>WARMUP_RATIO</code> float 0.1 The ratio of (wamrup learning rate)/(real learning rate) <code>WARMUP_ITERS</code> int 50 The number of warmup iterations <code>MILESTONES</code> list [] List of epoch indices in increagin order where the LR changes (exclusive parameter for MultiStepLR scheduler) <code>GAMMA</code> float 0.1 Multiplicative factor of learning rate decay. <code>amp</code> bool False Automatic mixed precision training. <code>auto_scale_lr</code> bool False Enable automatic scaling of learning rates. <code>cfg_options</code> bool or dict None Additional configuration options for the MMDET model. If True, it indicates the default options should be used. <code>train_ann_file</code> str 'train.json' Annotation file for training in COCO format. <code>val_ann_file</code> str 'val.json' Annotation file for validation in COCO format. <code>checkpoint_interval</code> int 5 Interval for saving checkpoints during training (in epochs)."},{"location":"nyuntam_adapt/algorithms/#instance-segmentation","title":"Instance Segmentation","text":"Parameter Datatype Default Value Description <code>BEGIN</code> int 0 The epoch from which the learning rate scheduler starts <code>END</code> int 50 The epoch at which the learning rate scheduler stops <code>T_MAX</code> int 0 Maximum number of iterations. (Exclusive Parameter for CossineAnnealingLR scheduler) <code>WARMUP</code> bool false Flag to indicate whether to use wamrup iters or not <code>WARMUP_RATIO</code> float 0.1 The ratio of (wamrup learning rate)/(real learning rate) <code>WARMUP_ITERS</code> int 50 The number of warmup iterations <code>MILESTONES</code> list [] List of epoch indices in increagin order where the LR changes (exclusive parameter for MultiStepLR scheduler) <code>GAMMA</code> float 0.1 Multiplicative factor of learning rate decay. <code>amp</code> bool False Automatic mixed precision training. <code>auto_scale_lr</code> bool False Enable automatic scaling of learning rates. <code>cfg_options</code> bool or dict None Additional configuration options for the MMDET model. If True, it indicates the default options should be used. <code>train_ann_file</code> str 'train.txt' Annotation file for training containing all the image names in train folder (without the extension) <code>val_ann_file</code> str 'val.txt' Annotation file for training containing all the image names in test folder (without the extension) <code>checkpoint_interval</code> int 5 Interval for saving checkpoints during training (in epochs). <code>class_list</code> list [] List containing all the classes in the segmentation task <code>palette</code> list [] List of Lists containing the RGB value for each class"},{"location":"nyuntam_adapt/algorithms/#pose-detection","title":"Pose Detection","text":"Parameter Datatype Default Value Description <code>BEGIN</code> int 0 The epoch from which the learning rate scheduler starts <code>END</code> int 50 The epoch at which the learning rate scheduler stops <code>T_MAX</code> int 0 Maximum number of iterations. (Exclusive Parameter for CossineAnnealingLR scheduler) <code>WARMUP</code> bool false Flag to indicate whether to use wamrup iters or not <code>WARMUP_RATIO</code> float 0.1 The ratio of (wamrup learning rate)/(real learning rate) <code>WARMUP_ITERS</code> int 50 The number of warmup iterations <code>MILESTONES</code> list [] List of epoch indices in increagin order where the LR changes (exclusive parameter for MultiStepLR scheduler) <code>GAMMA</code> float 0.1 Multiplicative factor of learning rate decay. <code>amp</code> bool False Automatic mixed precision training. <code>auto_scale_lr</code> bool False Enable automatic scaling of learning rates. <code>cfg_options</code> bool or dict None Additional configuration options for the MMDET model. If True, it indicates the default options should be used. <code>train_ann_file</code> str 'annotations/person_keypoints_val2017.json' Annotation file for training in COCO-Pose format. <code>val_ann_file</code> str 'annotations/person_keypoints_val2017.json' Annotation file for validation in COCO-Pose format. <code>checkpoint_interval</code> int 5 Interval for saving checkpoints during training (in epochs)."},{"location":"nyuntam_text_generation/","title":"Nyuntam Text Generation","text":""},{"location":"nyuntam_text_generation/#overview","title":"Overview","text":"<p>Nyuntam Text Generation evelops the SoTA compression methods and algorithms to achieve efficiency on text-generation tasks. This module implements model efficiency mixins via</p> <ul> <li>pruning</li> <li>quantization</li> <li>accelerations engines</li> </ul>"},{"location":"nyuntam_text_generation/#basic-workflow","title":"Basic Workflow","text":""},{"location":"nyuntam_text_generation/#step-1-import-a-model-and-dataset","title":"Step 1 - Import a model and dataset","text":"<p>Check Import Data for the exact steps. Make sure that the model-dataset combination provided is valid and of the same task.</p>"},{"location":"nyuntam_text_generation/#step-2-choose-an-algorithm","title":"Step 2 - Choose an Algorithm","text":"<p>Check available Algorithms and respective hyperparameters. By default, most optimal hyperparameters are chosen, however, depending upon the task, model and dataset another set of hyperparameters can work better too.</p>"},{"location":"nyuntam_text_generation/#step-3-monitor-logs-and-export","title":"Step 3 - Monitor Logs and Export","text":"<p>By default, Job logs and model checkpoints are saved in nyuntam_text_generation/user_data/, however, users can view the logs and download their compressed models locally using the export functionality.</p>"},{"location":"nyuntam_text_generation/algorithms/","title":"Nyuntam Text Generation","text":""},{"location":"nyuntam_text_generation/algorithms/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Nyuntam Text Generation</li> <li>Overview</li> <li>LLM Structured Pruning<ul> <li>Fluctuation-based Adaptive Structured Pruning (FLAP)</li> </ul> </li> <li>LLM Quantization<ul> <li>W4A16 Activation aware Weight-Quantization (AWQ)</li> <li>W4A8KV4 Quattuor-octo-Quattuor (QoQ)</li> <li>W2A16 Additive Quantization of Language Models (AQLM)</li> </ul> </li> <li>LLM Engine<ul> <li>TensorRT<ul> <li>Model/Quantization Support Grid</li> </ul> </li> <li>ExLlama</li> <li>MLCLLM</li> </ul> </li> </ul>"},{"location":"nyuntam_text_generation/algorithms/#overview","title":"Overview","text":"<p>Nyuntam Text Generation is a comprehensive suite of tools and algorithms designed to optimize and accelerate the inference of large language models (LLMs) for text generation tasks. The suite encompasses various techniques such as structured pruning, quantization, and engine optimization, all aimed at enhancing the efficiency of LLMs. These tools are compatible with popular models like GPT-2, GPT-3, and BERT and can be deployed across a wide range of platforms, including CPUs, GPUs, and edge devices.</p>"},{"location":"nyuntam_text_generation/algorithms/#llm-structured-pruning","title":"LLM Structured Pruning","text":""},{"location":"nyuntam_text_generation/algorithms/#fluctuation-based-adaptive-structured-pruning-flap","title":"Fluctuation-based Adaptive Structured Pruning (FLAP)","text":"<p>FLAP is an innovative framework that enhances the efficiency of Large Language Models (LLMs) by reducing storage requirements and improving inference speed. This framework outputs <code>model.safetensors</code>, which can be directly loaded using <code>load_and_replace_weights</code>.</p> <p>Parameters</p> Parameter Values Description Default Value pruning_ratio Float Pruning ratio 0.5 metrics \"WIFV\" Importance metric:\"WIFV\" (Weighted Importance Feature Value) \"WIFV\" structure \"AL-AM\" Pruning structure:\"AL-AM\" (Adaptive across both Layers and Modules) \"AL-MM\" remove_heads Int Number of heads to remove -1 nsamples Int Number of samples for evaluation 2048 start_pruning_layer_idx Int Decoder Layer index to start pruning from. 22"},{"location":"nyuntam_text_generation/algorithms/#llm-quantization","title":"LLM Quantization","text":""},{"location":"nyuntam_text_generation/algorithms/#w4a16-activation-aware-weight-quantization-awq","title":"W4A16 Activation aware Weight-Quantization (AWQ)","text":"<p>LLM Quantization involves a 4-bit weight-only quantization method specifically designed for Language Model (LM) applications. This method uses GEMM (General Matrix Multiply) as the default operation. The process generates <code>*.safetensor</code> and <code>config.json</code> files that can be directly loaded by transformers' <code>AutoModelForCausalLM.from_pretrained()</code> or AutoAWQ's <code>AutoAWQForCausalLM.from_quantized()</code> for quantized models.</p> <p>Parameters</p> Parameter Values Description Default Value zero_point bool Whether to use zero point. True q_group_size Int Quantization group size 128 w_bit Int Weight bitwidth (only 4-bit is supported) 4 version \"GEMM\", \"GEMV\" Version of AutoAWQ. One of GEMM or GEMV. \"GEMM\""},{"location":"nyuntam_text_generation/algorithms/#w4a8kv4-quattuor-octo-quattuor-qoq","title":"W4A8KV4 Quattuor-octo-Quattuor (QoQ)","text":"<p>QoQ employs a 4-bit weight, 8-bit activation, and 4-bit KV cache configuration. The algorithm includes a progressive quantization strategy to reduce dequantization overhead and a SmoothAttention mechanism to mitigate accuracy loss from 4-bit KV quantization.</p> <p>Parameters</p> Parameter Values Description Default Value save_model True Whether to save the model True keep_scales True Whether to keep scales during quantization True loads_with_qserve False Whether the model loads with QServe False quant_type \"gchn\",\"g128\",\"awq\",\"gptq\",\"sq_dynamic\",\"sq_static\" Quantization type -\"gchn\": QoQ Algorithm (Channelwise)\"g128\": QoQ Algorithm (Groupwise)\"awq\": AWQ Algorithm\"gptq\": GPTQ Algorithm\"sq_dynamic\": SmoothQuant Algorithm (Dynamic)\"sq_static\": SmoothQuant Algorithm (Static) \"gchn\" eval.tasks arc_challenge:25 Evaluation tasks arc_challenge:25 eval.max_seq_length 4096 Maximum sequence length for evaluation 4096 eval.evaluator \"lm_eval\" Evaluator used for evaluation \"lm_eval\" <p>Other nested config parameters can be updated as here. Find all the default configs for <code>quant_type</code> here.</p>"},{"location":"nyuntam_text_generation/algorithms/#w2a16-additive-quantization-of-language-models-aqlm","title":"W2A16 Additive Quantization of Language Models (AQLM)","text":"<p>AQLM introduces learned additive quantization, tailored to each transformer block, and jointly optimizes codebook parameters across blocks. It stands out for being Pareto-optimal in accuracy vs. model size for models compressed to less than 3 bits per parameter. AQLM also offers practical, fast implementations for GPU and CPU, making it suitable for deploying LLMs on end-user devices.</p> <p>Parameters</p> Parameter Values Description Default Value save_intermediate_results bool Whether to save intermediate results true dtype string Data type for quantization \"float16\" Calibration Config attn_implementation null or string Attention implementation null beam_size int Beam size for calibration 1 codebook_value_nbits int Number of bits for codebook values 16 codebook_value_num_groups int Number of groups for codebook values 1 dtype string Data type for calibration \"float16\" finetune_adam_beta1 float Adam beta1 for finetuning 0.9 finetune_adam_beta2 float Adam beta2 for finetuning 0.999 finetune_batch_size int Batch size for finetuning 16 finetune_early_stop int Early stopping criterion for finetuning 3 finetune_keep_best bool Whether to keep the best model during finetuning true finetune_lr float Learning rate for finetuning 0.0001 finetune_max_epochs int Maximum number of epochs for finetuning 25 in_group_size int Input group size 8 init_max_iter int Maximum iterations for initialization 100 init_max_points_per_centroid null or int Maximum points per centroid for initialization null local_batch_size int Local batch size 1 lr float Learning rate 0.0001 max_epochs int Maximum number of epochs 100 mix_compression bool Whether to use mixed compression false model_seqlen int Model sequence length 4096 nbits_per_codebook int Number of bits per codebook 16 new_eval bool Whether to use new evaluation false no_quant bool Whether to disable quantization false nsamples int Number of samples 2048 num_codebooks int Number of codebooks 1 offload_activations bool Whether to offload activations true on_save null or function Function to call on save null out_group_size int Output group size 1 print_frequency int Frequency of printing 10 relative_mse_tolerance float Relative MSE tolerance 0.01 resume bool Whether to resume training false scale_nbits int Number of bits for scaling 0 seed int Random seed 0 skip_out_loss bool Whether to skip output loss false steps_per_epoch int Steps per epoch 100 true_sequential bool Whether to use true sequential processing false trust_remote_code bool Whether to trust remote code true use_checkpointing bool Whether to use checkpointing false use_faiss bool Whether to use Faiss false use_fast_tokenizer bool Whether to use fast tokenizer false val_size int Validation size 256 wandb bool Whether to use Weights &amp; Biases false Finetune Config adam_beta1 float Adam beta1 for finetuning 0.9 adam_beta2 float Adam beta2 for finetuning 0.95 amp_dtype string AMP data type float32 amsgrad bool Whether to use AMSGrad false attn_implementation null or string Attention implementation for finetuning null base_model string Base model name base_model batch_size int Batch size 1 beam_size int Beam size 1 block_type string Block type LlamaDecoderLayer code_adam_16bit bool Whether to use 16-bit Adam for codes false code_beta1 float Beta1 for code optimization 0.0 code_beta2 float Beta2 for code optimization 0.95 code_dtype string Data type for codes uint16 code_lr float Learning rate for codes 0.001 code_selection_temperature float Temperature for code selection 0 code_trust_ratio float Trust ratio for codes 0.01 debias bool Whether to debias true delta_decay float Delta decay 0 download_num_workers null or int Number of workers for downloading null eval_datasets list Evaluation datasets [\"wikitext2\", \"c4\"] eval_every_steps int Evaluate every n steps 1 force_code_update bool Whether to force code update false gradient_checkpointing bool Whether to use gradient checkpointing true keep_best_model bool Whether to keep the best model false lamb bool Whether to use LAMB optimizer true limit_parallel_inits int Limit on parallel initializations 1 load_dtype string Data type for loading float32 lr float Learning rate 0.0001 master_dtype string Master data type float32 max_code_change_per_step float Maximum code change per step 0.01 max_epochs int Maximum number of epochs 10 microbatch_size int Microbatch size 1 minimize_sync bool Whether to minimize synchronization false model_seqlen int Model sequence length 4096 monkeypatch_old_pickle bool Whether to monkeypatch old pickle false num_workers int Number of workers 8 overwrite_cache bool Whether to overwrite cache false preprocessing_chunk_length null or int Preprocessing chunk length null preprocessing_keep_in_memory bool Whether to keep preprocessing in memory false preprocessing_num_workers int Number of preprocessing workers 24 print_every_steps int Print every n steps 1 save_every_steps int Save every n steps 1 seed int Random seed 1337 straight_through_buffer_dtype string Straight-through buffer data type float32 trust_remote_code bool Whether to trust remote code true update_codebooks_and_scales bool Whether to update codebooks and scales true update_codes bool Whether to update codes true update_non_quantized_parameters bool Whether to update non-quantized parameters true use_fast_tokenizer bool Whether to use fast tokenizer false use_fsdp_amp bool Whether to use FSDP AMP false verbose_optimizer bool Whether to use verbose optimizer true wandb bool Whether to use Weights &amp; Biases false wrap_separately list Layers to wrap separately [] Conversion Config attn_implementation null or string Attention implementation for conversion null code_dtype string Data type for codes int32 load_dtype string Data type for loading auto trust_remote_code bool Whether to trust remote code for conversion true"},{"location":"nyuntam_text_generation/algorithms/#llm-engine","title":"LLM Engine","text":""},{"location":"nyuntam_text_generation/algorithms/#tensorrt","title":"TensorRT","text":"<p>The LLM Engine TensorRT optimizes LLMs for inference by building TensorRT engines equipped with state-of-the-art optimizations to ensure efficient inference performance. The output is a <code>.engine</code> model file that can be directly loaded by NVIDIA Triton Inference Server.</p> <p>Parameters</p> Parameter Values Description Default Value to_quantize bool Whether to quantize the model before building the engine. True quant_method \"fp8\", \"int4_awq\", \"smoothquant\", \"int8\" Quantization format \"int4_awq\" smoothquant float (if quant_method = \"smoothquant\") The smooth quant's \u03b1 value, which controls quantization difficulty migration between activations and weights. 0.5 calib_size Int Calibration size 32 dtype \"float16\" The data type of the model \"float16\""},{"location":"nyuntam_text_generation/algorithms/#modelquantization-support-grid","title":"Model/Quantization Support Grid","text":"Model fp8 int4_awq smoothquant int8 LLaMA \u2713 \u2713 \u2713 \u2713 LLaMA-2 \u2713 \u2713 \u2713 \u2713 Vicuna \u2713 \u2713 \u2713 \u2713 Mixtral \u2713 \u2713 - \u2713 Mistral-7B \u2713 \u2713 - \u2713 Gemma \u2713 \u2713 - \u2713"},{"location":"nyuntam_text_generation/algorithms/#exllama","title":"ExLlama","text":"<p>LLM Engine ExLlama introduces a new quantization format known as EXL2, providing flexibility in weight storage. This implementation generates engine files and a script for fast inference on the given model. The output includes <code>.safetensor</code>, <code>config.json</code> model files, and a <code>run.sh</code> script for test inference using ExllamaV2.</p> <p>Parameters</p> Parameter Values Description Default Value bits Float &gt;= 2 , &lt;= 8 Target bits per weight 4.125 shard_size Int Maximum shard size in MB while saving the model 8192 rope_scale Float RoPE scaling factor (related to RoPE (NTK) parameters) 1 rope_alpha Float RoPE alpha value (related to RoPE (NTK) parameters) 1 head_bits Int Target bits per weight (for the head layer) 6"},{"location":"nyuntam_text_generation/algorithms/#mlcllm","title":"MLCLLM","text":"<p>The LLM Engine MLCLLM offers compiler accelerations and runtime optimizations for native deployment across various platforms, including edge devices. The output consists of <code>params-*.bin</code> files and compiled files that can be directly used by MLC Chat, along with a <code>run.py</code> script for sample usage.</p> <p>Parameters</p> Parameter Values Description Default Value quantize bool Indicates whether quantization is applied to the model True quant_method \"q4f16_0\", \"q4f16_autoawq\" Method used for quantization \"q4f16_autoawq\" conv_template \"llama-2\" Conversation templates None llvm_triple null LLVM triple None"},{"location":"nyuntam_vision/","title":"Nyuntam Vision","text":""},{"location":"nyuntam_vision/#overview","title":"Overview","text":"<p>Developed to compress and optimize deep learning models, Nyuntam Vision provides a set of compression techniques tailored for specific deployment constraints. Users have the flexibility to choose and combine multiple techniques to achieve the best trade-off between model performance and deployment constraints. Leveraging cutting-edge techniques like pruning, quantization, distillation, etc. It achieves exceptional model compression levels on a variety of vision models.</p>"},{"location":"nyuntam_vision/#basic-workflow","title":"Basic Workflow","text":""},{"location":"nyuntam_vision/#step-1-import-a-model-and-dataset","title":"Step 1 - Import a model and dataset","text":"<p>Check Import Data for the exact steps. Make sure that the model-dataset combination provided is valid and of the same task.</p>"},{"location":"nyuntam_vision/#step-2-choose-an-algorithm","title":"Step 2 - Choose an Algorithm","text":"<p>Check available Algorithms and respective hyperparameters. By default, most optimal hyperparameters are chosen, however, depending upon the task, model and dataset another set of hyperparameters can work better too.</p>"},{"location":"nyuntam_vision/#step-3-monitor-logs-and-export","title":"Step 3 - Monitor Logs and Export","text":"<p>By default, Job logs and model checkpoints are saved in nyuntam_vision/user_data/, however, users can view the logs and download their compressed models locally using the export functionality.</p>"},{"location":"nyuntam_vision/algorithms/","title":"Algorithms","text":""},{"location":"nyuntam_vision/algorithms/#overview","title":"Overview","text":"<p>Nyuntam Vision currently supports the following tasks and respective algorithms -</p> <ul> <li>Image Classification -<ul> <li>CPU Post Training Quantization - Torch</li> <li>CPU Post Training Quantization - OpenVino</li> <li>CPU Post Training Quantization - ONNX</li> <li>GPU Post Training Quantization - TensorRT</li> <li>CPU Quantization Aware Training - Torch</li> <li>Knowledge Distillation</li> <li>Structured Pruning</li> </ul> </li> <li>Object Detection -<ul> <li>CPU Post Training Quantization - ONNX</li> <li>GPU Post Training Quantization - TensorRT</li> <li>CPU Quantization Aware Training - OpenVino</li> <li>Knowledge Distillation</li> <li>Structured Pruning</li> </ul> </li> </ul>"},{"location":"nyuntam_vision/algorithms/#vision-compressions","title":"Vision Compressions","text":""},{"location":"nyuntam_vision/algorithms/#cpu-post-training-quantization-torch","title":"CPU Post Training Quantization - Torch","text":"<p>Native CPU quantization. 8 bit quantization by default. Outputs <code>.pt</code> model file which can be directly loaded by <code>torch.load</code>. </p> Parameter Values Description Default Value insize Int Input Shape For Vision Tasks (Currently only A X A Shapes supported) 32 BATCH_SIZE Int Batch Size 1 TRAINING bool Enables Finetuning before PTQ True VALIDATE bool Enables Validation during Optional Finetuning (When TRAINING=True) True VALIDATION_INTERVAL Int Defines Epoch Intervals for Validation during Finetuning (When TRAINING, VALIDATE = True) 1 CRITERION \"CrossEntropyLoss\",\"MSE Loss\",others Defines Loss functions for finetuning/validation (When TRAINING = True) \"CrossEntropyLoss\" LEARNING RATE Float Defines Learning Rate for Finetuning (When TRAINING=True) 0.001 FINETUNE_EPOCHS Int Defines the number of Epochs for Finetuning (When TRAINING=True) 1 OPTIMIZER \"Adam\",\"SGD\",others Defines Optimizer for Finetuning. (When TRAINING = TRUE) Adam PRETRAINED bool Indicates whether to load ImageNet Weights in case custom model is not provided. False choice \"static\",\"weight\" or \"fusion\" Indicates the Kind of PTQ to be performed. \"static\""},{"location":"nyuntam_vision/algorithms/#cpu-post-training-quantization-openvino","title":"CPU Post Training Quantization - OpenVino","text":"<p>Neural networks inference optimization in OpenVINORuntime with minimal accuracy drop. Outputs <code>.xml</code> and <code>.bin</code> model files which can be directly loaded by <code>openvino.core.read_model</code>.</p> Parameter Values Description Default Value insize Int Input Shape For Vision Tasks (Currently only A X A Shapes supported) 32 BATCH_SIZE Int Batch Size 1 TRAINING bool Enables Finetuning before PTQ True VALIDATE bool Enables Validation during Optional Finetuning (When TRAINING = True) True VALIDATION_INTERVAL Int Defines Epoch Intervals for Validation during Finetuning (When TRAINING, VALIDATE = True) 1 CRITERION \"CrossEntropyLoss\",\"MSE Loss\",others Defines Loss functions for finetuning/validation (When TRAINING = True) CrossEntropyLoss LEARNING RATE Float Defines Learning Rate for Finetuning (When TRAINING = True) 0.001 FINETUNE_EPOCHS Int Defines the number of Epochs for Finetuning (When TRAINING = True) 1 OPTIMIZER \"Adam\",\"SGD\",others Defines Optimizer for Finetuning. (When TRAINING = TRUE) Adam PRETRAINED bool Indicates whether to load ImageNet Weights in case custom model is not provided. False TRANSFORMER bool Indicates whether uploaded model consists a transformer based architecture (Only For Classification) True"},{"location":"nyuntam_vision/algorithms/#cpu-post-training-quantization-onnx","title":"CPU Post Training Quantization - ONNX","text":"<p>ONNX 8-bit CPU Post Training Quantization for Pytorch models. Outputs <code>.onnx</code> model files which can be directly loaded by <code>onnx.load</code>. </p> Parameter Values Description Default Value insize Int Input Shape For Vision Tasks (Currently only A X A Shapes supported) 32 BATCH_SIZE Int Batch Size for dataloader 1 TRAINING bool Enables Finetuning before PTQ True VALIDATE bool <p>Enables Validation during Optional Finetuning</p><p>(When TRAINING = True)</p> True VALIDATION_INTERVAL Int <p>Defines Epoch Intervals for Validation during Finetuning.</p><p>(When TRAINING, VALIDATE = True)</p> 1 CRITERION <p>\u201cCrossEntropyLoss\u201d,</p><p>\u201cMSE Loss\u201d,</p><p>others</p> <p>Defines Loss functions for finetuning/validation</p><p>(When TRAINING = True)</p> CrossEntropyLoss LEARNING RATE Float Defines Learning Rate for Finetuning (When TRAINING = True) 0.001 FINETUNE_EPOCHS Int <p>Defines the number of Epochs for Finetuning</p><p>(When TRAINING = True)</p> 1 OPTIMIZER <p>\u201cAdam\u201d,</p><p>\u201cSGD\u201d,</p><p>others </p> Defines Optimizer for Finetuning. (When TRAINING = True) Adam PRETRAINED bool Indicates whether to load ImageNet Weights in case custom model is not provided. False quant_format QuantFormat.QDQ, QuantFormat.QOperator Indicates the ONNX quantization representation format QuantFormat.QDQ per_channel bool Indicates usage of \"Per Channel\" quantization that improves accuracy of models with large weight range False activation_type <p>QuantType.QInt8, QuantType.QUInt8, QuantType.QFLOAT8E4M3FN, QuantType.QInt16, QuantType.QUInt16</p><p></p> Indicates the expected data type of activations post quantization QuantType.QInt8 weight_type <p>QuantType.QInt8, QuantType.QUInt8, QuantType.QFLOAT8E4M3FN, QuantType.QInt16, QuantType.QUInt16</p><p></p> Indicates the expected data type of weights post quantization QuantType.QInt8"},{"location":"nyuntam_vision/algorithms/#cpu-quantization-aware-training-torch","title":"CPU Quantization Aware Training - Torch","text":"<p>Native CPU quantization. 8 bit quantization by default. Outputs <code>.pt</code> model file which can be directly loaded by <code>torch.load</code>. </p> Parameter Values Description Default Value insize Int Input Shape For Vision Tasks (Currently only A X A Shapes supported) 32 BATCH_SIZE Int Batch Size 1 TRAINING bool Enables Finetuning before PTQ True VALIDATE bool Enables Validation during Optional Finetuning (When TRAINING=True) True VALIDATION_INTERVAL Int Defines Epoch Intervals for Validation during Finetuning (When TRAINING, VALIDATE = True) 1 CRITERION \"CrossEntropyLoss\",\"MSE Loss\",others Defines Loss functions for finetuning/validation (When TRAINING = True) \"CrossEntropyLoss\" LEARNING RATE Float Defines Learning Rate for Finetuning (When TRAINING=True) 0.001 FINETUNE_EPOCHS Int Defines the number of Epochs for Finetuning (When TRAINING=True) 1 OPTIMIZER \"Adam\",\"SGD\",others Defines Optimizer for Finetuning. (When TRAINING = TRUE) Adam PRETRAINED bool Indicates whether to load ImageNet Weights in case custom model is not provided. False"},{"location":"nyuntam_vision/algorithms/#cpu-quantization-aware-training-openvino","title":"CPU Quantization Aware Training - OpenVino","text":"<p>Neural networks inference optimization in OpenVINORuntime with minimal accuracy drop. Outputs <code>.xml</code> and <code>.bin</code> model files which can be directly loaded by <code>openvino.core.read_model</code>.</p> Parameter Values Description Default Value insize Int Input Shape For Vision Tasks (Currently only A X A Shapes supported) 32 BATCH_SIZE Int Batch Size 1 TRAINING bool Enables Finetuning before PTQ True VALIDATE bool Enables Validation during Optional Finetuning (When TRAINING = True) True VALIDATION_INTERVAL Int Defines Epoch Intervals for Validation during Finetuning (When TRAINING, VALIDATE = True) 1 CRITERION \"CrossEntropyLoss\",\"MSE Loss\",others Defines Loss functions for finetuning/validation (When TRAINING = True) CrossEntropyLoss LEARNING RATE Float Defines Learning Rate for Finetuning (When TRAINING = True) 0.001 FINETUNE_EPOCHS Int Defines the number of Epochs for Finetuning (When TRAINING = True) 1 OPTIMIZER \"Adam\",\"SGD\",others Defines Optimizer for Finetuning. (When TRAINING = TRUE) Adam PRETRAINED bool Indicates whether to load ImageNet Weights in case custom model is not provided. False TRANSFORMER bool Indicates whether uploaded model consists a transformer based architecture True"},{"location":"nyuntam_vision/algorithms/#gpu-post-training-quantization-tensorrt","title":"GPU Post Training Quantization - TensorRT","text":"<p>8-bit Quantization executable in GPU via TensorRT Runtime. Outputs <code>.engine</code> model file which can be directly loaded by <code>tensorrt.Runtime</code>. </p> Parameter Values Description Default Value insize Int Input Shape For Vision Tasks (Currently only A X A Shapes supported) 32 BATCH_SIZE Int Batch Size for dataloader 1 TRAINING bool Enables Finetuning before PTQ True VALIDATE bool <p>Enables Validation during Optional Finetuning</p><p>(When TRAINING = True)</p> True VALIDATION_INTERVAL Int <p>Defines Epoch Intervals for Validation during Finetuning.</p><p>(When TRAINING, VALIDATE = True)</p> 1 CRITERION <p>\u201cCrossEntropyLoss\u201d,</p><p>\u201cMSE Loss\u201d,</p><p>others</p> <p>Defines Loss functions for finetuning/validation</p><p>(When TRAINING = True)</p> CrossEntropyLoss LEARNING RATE Float Defines Learning Rate for Finetuning (When TRAINING = True) 0.001 FINETUNE_EPOCHS Int <p>Defines the number of Epochs for Finetuning</p><p>(When TRAINING = True)</p> 1 OPTIMIZER <p>\u201cAdam\u201d,</p><p>\u201cSGD\u201d,</p><p>others </p> Defines Optimizer for Finetuning. (When TRAINING = TRUE) Adam PRETRAINED bool Indicates whether to load ImageNet Weights in case custom model is not provided. False"},{"location":"nyuntam_vision/algorithms/#knowledge-distillation","title":"Knowledge Distillation","text":"<p>Simple Distillation Training Strategy that adds an additional loss between Teacher and Student Predictions. Outputs <code>.pt</code> model file which can be directly loaded by using <code>torch.load</code>.</p> Parameter Values Description Default Value insize Int A single integer representing the input image size for teacher network and student network 32 BATCH_SIZE Int Batch Size for dataloader 1 TRAINING bool Whether to finetune teacher model before distillation. True VALIDATE bool <p>Enables Validation during Optional Finetuning</p><p>(When TRAINING = True)</p> True VALIDATION_INTERVAL Int <p>Defines Epoch Intervals for Validation during Finetuning.</p><p>(When TRAINING, VALIDATE = True)</p> 1 CRITERION <p>\u201cCrossEntropyLoss\u201d,</p><p>\u201cMSE Loss\u201d,</p><p>others</p> <p>Defines Loss functions for finetuning/validation</p><p>(When TRAINING = True)</p> CrossEntropyLoss LEARNING RATE Float Defines Learning Rate for Finetuning (When TRAINING = True) 0.001 FINETUNE_EPOCHS Int <p>Defines the number of Epochs for Finetuning</p><p>(When TRAINING = True)</p> 1 OPTIMIZER <p>\u201cAdam\u201d,</p><p>\u201cSGD\u201d,</p><p>others </p> Defines Optimizer for Finetuning. (When TRAINING = TRUE) Adam TEACHER_MODEL String Model Name of the provided Teacher Model. (Required both when intrinsicly provided and when custom teacher is uploaded) vgg16 CUSTOM_TEACHER_PATH String Relative Path for Teacher checkpoint from User Data folder None METHOD \"pkd\",\"cwd\",\"pkd_yolo\" Distillation Algorithm to use to distill models.  Needed for MMDetection (pkd,cwd) and MMYolo(pkd_yolo) distillation. Not needed for classification. pkd EPOCHS Int Indicates Number of Training Epochs for Distillation 20 LR Float Indicates Learning Rate for distillation process. 0.01 LAMBDA Float Adjusts the balance between cross entropy andKLDiv (Classification Only) 0.5 TEMPERATURE Int Indicates Temperature for softmax (Classification Only) 20 SEED Int Sets the seed for random number generation (Classification Only) 43 WEIGHT_DECAY Float Sets the amount of Weight Decay during Distillation (Classification Only) 0.0005"},{"location":"nyuntam_vision/algorithms/#structured-pruning-image-classification","title":"Structured Pruning (Image Classification)","text":"<p>Pruning existing Parameters to increase Efficiency. MM Detection and MM Segmentation models are currently supported through MM Razor Pruning Algorithms. Outputs <code>.pt</code> model file which can be directly loaded by <code>torch.load</code>. </p> Parameter Values Description Default Value insize Int Input Shape For Vision Tasks (Currently only A X A Shapes supported) 32 BATCH_SIZE Int Batch Size for dataloader 1 TRAINING bool Whether to finetune model after pruning. True VALIDATE bool <p>Enables Validation during Optional Finetuning</p><p>(When TRAINING = True)</p> True VALIDATION_INTERVAL Int <p>Defines Epoch Intervals for Validation during Finetuning.</p><p>(When TRAINING, VALIDATE = True)</p> 1 CRITERION <p>\u201cCrossEntropyLoss\u201d,</p><p>\u201cMSE Loss\u201d,</p><p>others</p> <p>Defines Loss functions for finetuning/validation</p><p>(When TRAINING = True)</p> CrossEntropyLoss LEARNING RATE Float Defines Learning Rate for Finetuning (When TRAINING = True) 0.001 FINETUNE_EPOCHS Int <p>Defines the number of Epochs for Finetuning</p><p>(When TRAINING = True)</p> 1 OPTIMIZER <p>\u201cAdam\u201d,</p><p>\u201cSGD\u201d,</p><p>others </p> Defines Optimizer for Finetuning. (When TRAINING = TRUE) Adam PRETRAINED bool Indicates whether to load ImageNet Weights in case custom model is not provided. False <p>The below parameters are specifically for the pruning of classification models.</p> Parameter Values Description Default Value PRUNER_NAME <p>MetaPruner,</p><p>GroupNormPruner,</p><p>BNSPruner,</p> <p>Pruning Algorithm to be utilized for pruning classification models.</p> GroupNormPruner GROUP_IMPORTANCE <p>GroupNormImportance,</p><p>GroupTaylorImportance</p> <p>Logic for identify importance of parameters to prune.</p> <p>GroupNormImportance</p> TARGET_PRUNE_RATE Int Parameter Reduction Rate, defines how much parameters are reduced Integer Value BOTTLENECK bool <p>When Pruning Transformer based Architectures, whether to prune only intermediate layers (bottleneck) or perform uniform pruning.</p> <p>False</p> PRUNE_NUM_HEADS bool Whether to Prune number of Heads (For Transformer based Architectures) True"},{"location":"nyuntam_vision/algorithms/#structured-pruning-object-detection","title":"Structured Pruning (Object Detection)","text":"<p>The below parameters are specifically for the pruning of object detection models.</p> Parameter Values Description Default Value INTERVAL Int Epoch Interval between every pruning operation 10 NORM_TYPE \"act\", \"flops\" Type of pruning operation. \"act\" focuses on reducing parameters with minimal changes to activations. \"flops\" focuses on improving number of flops. \"act\" LR_RATIO Float Ratio to decrease lr rate. 0.1 TARGET_FLOP_RATIO Float The target flop ratio to prune your model. (also used for \"act\"). 0.5 EPOCHS Int Number of epochs to perform training (possibly a multiple of Interval). 20"}]}